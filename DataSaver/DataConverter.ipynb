{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import SentenceReport, DocumentReport\n",
    "from utils.nlp_utils import NLPUtils\n",
    "from utils.io_utils import IOUtils\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OwnDataCreator:\n",
    "    def load_sentences(infile, permission, stemmer, embeddings):\n",
    "        print(\"Loading row {} \".format(infile))\n",
    "        tagged_train_file = pd.read_csv(infile)\n",
    "        sentences = []\n",
    "\n",
    "        app_id = None\n",
    "        for idx, row in tagged_train_file.iterrows():\n",
    "            sentence_id = str(row[\"app_id\"])\n",
    "            sentence = row[\"sentence\"]\n",
    "            if sentence_id.startswith(\"#\"):\n",
    "                app_id = sentence_id\n",
    "            if not (sentence.startswith(\"##\") or sentence.startswith(\"Description Tag\") or sentence.startswith(\"CATEGORY\")):\n",
    "                try:\n",
    "                    if int(row[permission]) == 1 or int(row[permission]) == 0: #eliminate different tags other than zero and one       \n",
    "                        sentence_report = SentenceReport(app_id, sentence)\n",
    "                        sentence_report.permissions[permission] = int(row[permission])\n",
    "                        preprocessed = NLPUtils.preprocess_sentence(sentence, stemmer)\n",
    "                        sentence_report.preprocessed_sentence = [word for word in preprocessed if word in embeddings]\n",
    "                        if sentence_report.preprocessed_sentence != []:\n",
    "                            sentences.append(sentence_report)\n",
    "                    else:\n",
    "                        pass\n",
    "                        # Pass tags other than zero and one \n",
    "                except:\n",
    "                    pass # sentences with no tag\n",
    "        print(\"Loading completed\")\n",
    "        return sentences\n",
    "    \n",
    "    def load_documents(infile, permission, stemmer, embeddings):\n",
    "        print(\"Loading row {} \".format(infile))\n",
    "        tagged_train_file = pd.read_csv(infile)\n",
    "        documents = []\n",
    "\n",
    "        app_id = None\n",
    "        for idx, row in tagged_train_file.iterrows():\n",
    "            sentence_id = str(row[\"app_id\"])\n",
    "            sentence = row[\"sentence\"]\n",
    "            if sentence_id.startswith(\"#\"):\n",
    "                app_id = sentence_id\n",
    "                documents.append(DocumentReport(app_id))\n",
    "                documents[-1].permissions[permission] = 0\n",
    "            if not (sentence.startswith(\"##\") or sentence.startswith(\"Description Tag\") or sentence.startswith(\"CATEGORY\")):\n",
    "                if not (sentence.startswith(\"##\") or sentence.startswith(\"Description Tag\") or sentence.startswith(\"CATEGORY\")):\n",
    "                    try:\n",
    "                        if int(row[permission]) == 1 or int(row[permission]) == 0: #eliminate different tags other than zero and one       \n",
    "                            if row[permission] == 1:\n",
    "                                documents[-1].permissions[permission] = 1\n",
    "                            documents[-1].sentences.append(sentence)\n",
    "                            preprocessed = NLPUtils.preprocess_sentence(sentence, stemmer)\n",
    "                            filtered = [word for word in preprocessed if word in embeddings]\n",
    "                            if filtered:\n",
    "                                documents[-1].preprocessed_sentences.append(filtered)\n",
    "                        else:\n",
    "                            pass\n",
    "                            # Pass tags other than zero and one \n",
    "                    except:\n",
    "                        pass # sentences with no tag\n",
    "        non_empty_documents = []\n",
    "        for doc in documents:\n",
    "            if len(doc.preprocessed_sentences) != 0:\n",
    "                non_empty_documents.append(doc)\n",
    "                \n",
    "        print(\"Loading completed\")\n",
    "        return non_empty_documents\n",
    "\n",
    "    def vocab(infile, permission, stemmer, embeddings):\n",
    "        print(\"Loading row {} \".format(infile))\n",
    "        tagged_train_file = pd.read_csv(infile)\n",
    "        w2i = {}\n",
    "        for idx, row in tagged_train_file.iterrows():\n",
    "            sentence = row[\"sentence\"]\n",
    "\n",
    "            if not (sentence.startswith(\"##\") or sentence.startswith(\"Description Tag\") or sentence.startswith(\"CATEGORY\")):\n",
    "                try:\n",
    "                    if int(row[permission]) == 1 or int(row[permission]) == 0: #eliminate different tags other than zero and one       \n",
    "                        preprocessed = NLPUtils.preprocess_sentence(sentence, stemmer)\n",
    "                        filtered = [word for word in preprocessed if word in embeddings]\n",
    "                        for token in filtered:\n",
    "                            if token not in w2i:\n",
    "                                w2i[token] = len(w2i)\n",
    "                    else:\n",
    "                        pass\n",
    "                        # Pass tags other than zero and one \n",
    "                except:\n",
    "                    pass # sentences with no tag\n",
    "        print(\"Loading completed\")\n",
    "        return w2i\n",
    "\n",
    "    def filtered_vocab_embeddings(w2i, embeddings):\n",
    "        subset = {}\n",
    "        for key in w2i:\n",
    "            subset[key] = embeddings[key]\n",
    "        return subset\n",
    "\n",
    "    def save_sentence_based_dataset(infile, permission, embeddings, stemmer, outfile):\n",
    "        sentences = OwnDataCreator.load_sentences(infile, permission, stemmer, embeddings)\n",
    "        w2i = OwnDataCreator.vocab(infile, permission, stemmer, embeddings)\n",
    "        subset_embeddings = OwnDataCreator.filtered_vocab_embeddings(w2i, embeddings)\n",
    "        with open(outfile, \"wb\") as target:\n",
    "            pickle.dump([subset_embeddings, sentences, w2i], target)\n",
    "\n",
    "    def save_document_based_dataset(infile, permission, embeddings, stemmer, outfile):\n",
    "        documents = OwnDataCreator.load_documents(infile, permission, stemmer, embeddings)\n",
    "        w2i = OwnDataCreator.vocab(infile, permission, stemmer, embeddings)\n",
    "        subset_embeddings = OwnDataCreator.filtered_vocab_embeddings(w2i, embeddings)\n",
    "        with open(outfile, \"wb\") as target:\n",
    "            pickle.dump([subset_embeddings, documents, w2i], target)\n",
    "    \n",
    "    def run():\n",
    "        \"\"\"\n",
    "        For fasttext embeddings,\n",
    "        stemmer =  \"nostemmer\"\n",
    "        embeddings_file = os.path.join(os.environ[\"SECURITY_DATASETS\"], \"embeddings/cc.en.300.bin\")\n",
    "        embeddings, embeddings_dim = IOUtils.load_embeddings_file(embeddings_file, \"fasttext\", lower=True)\n",
    "        #sentences\n",
    "        outfile = os.path.join(os.environ[\"SECURITY_DATASETS\"], \"saved-parameters/saved-data/saved-data/own-data/{}-fasttext-embeddings-sentences-w2i.pickle\".format(permission))\n",
    "        #documents\n",
    "        outfile = os.path.join(os.environ[\"SECURITY_DATASETS\"], \"saved-parameters/saved-data/saved-data/own-data/{}-fasttext-embeddings-documents-w2i.pickle\".format(permission))\n",
    "\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        For no stem own embeddings,\n",
    "        stemmer =  \"nostemmer\"\n",
    "        embeddings_file = os.path.join(os.environ[\"SECURITY_DATASETS\"], \"embeddings/own-embeddings/sscraped_no_stemming_300.bin\")\n",
    "        embeddings, embeddings_dim = IOUtils.load_embeddings_file(embeddings_file, \"word2vec\", lower=True)\n",
    "        #sentences\n",
    "        outfile = os.path.join(os.environ[\"SECURITY_DATASETS\"], \"saved-parameters/saved-data/saved-data/own-data/{}-nostem-embeddings-sentences-w2i.pickle\".format(permission))\n",
    "        #documents\n",
    "        outfile = os.path.join(os.environ[\"SECURITY_DATASETS\"], \"saved-parameters/saved-data/saved-data/own-data/{}-nostem-embeddings-documents-w2i.pickle\".format(permission))\n",
    "\n",
    "        \"\"\"\n",
    "        stemmer = \"porter\" \n",
    "        embeddings_file = os.path.join(os.environ[\"SECURITY_DATASETS\"], \"embeddings/own-embeddings/scraped_with_porter_stemming_300.bin\")\n",
    "        embeddings, embeddings_dim = IOUtils.load_embeddings_file(embeddings_file, \"word2vec\", lower=True)\n",
    "        \"\"\"\n",
    "        #create sentence based embeddings\n",
    "        permission = \"STORAGE\"\n",
    "        infile = os.path.join(os.environ[\"SECURITY_DATASETS\"], \"created-data/{}.csv\".format(permission))\n",
    "        outfile = os.path.join(os.environ[\"SECURITY_DATASETS\"],\"saved-parameters/saved-data/own-data/{}-embeddings-sentences-w2i.pickle\".format(permission))\n",
    "        OwnDataCreator.save_sentence_based_dataset(infile, permission, embeddings, stemmer, outfile)\n",
    "\n",
    "        permission = \"RECORD_AUDIO\"\n",
    "        infile = os.path.join(os.environ[\"SECURITY_DATASETS\"], \"created-data/{}.csv\".format(permission))\n",
    "        outfile = os.path.join(os.environ[\"SECURITY_DATASETS\"],\"saved-parameters/saved-data/own-data/{}-embeddings-sentences-w2i.pickle\".format(permission))\n",
    "        OwnDataCreator.save_sentence_based_dataset(infile, permission, embeddings, stemmer, outfile)\n",
    "\n",
    "        permission = \"READ_CONTACTS\"\n",
    "        infile = os.path.join(os.environ[\"SECURITY_DATASETS\"], \"created-data/{}.csv\".format(permission))\n",
    "        outfile = os.path.join(os.environ[\"SECURITY_DATASETS\"],\"saved-parameters/saved-data/own-data/{}-embeddings-sentences-w2i.pickle\".format(permission))\n",
    "        OwnDataCreator.save_sentence_based_dataset(infile, permission, embeddings, stemmer, outfile)\n",
    "        \"\"\"\n",
    "        #create document based embeddings\n",
    "        permission = \"STORAGE\"\n",
    "        infile = os.path.join(os.environ[\"SECURITY_DATASETS\"], \"created-data/{}.csv\".format(permission))\n",
    "        outfile = os.path.join(os.environ[\"SECURITY_DATASETS\"],\"saved-parameters/saved-data/own-data/{}-embeddings-documents-w2i.pickle\".format(permission))\n",
    "        OwnDataCreator.save_document_based_dataset(infile, permission, embeddings, stemmer, outfile)\n",
    "\n",
    "        permission = \"RECORD_AUDIO\"\n",
    "        infile = os.path.join(os.environ[\"SECURITY_DATASETS\"], \"created-data/{}.csv\".format(permission))\n",
    "        outfile = os.path.join(os.environ[\"SECURITY_DATASETS\"],\"saved-parameters/saved-data/own-data/{}-embeddings-documents-w2i.pickle\".format(permission))\n",
    "        OwnDataCreator.save_document_based_dataset(infile, permission, embeddings, stemmer, outfile)\n",
    "\n",
    "        permission = \"READ_CONTACTS\"\n",
    "        infile = os.path.join(os.environ[\"SECURITY_DATASETS\"], \"created-data/{}.csv\".format(permission))\n",
    "        outfile = os.path.join(os.environ[\"SECURITY_DATASETS\"],\"saved-parameters/saved-data/own-data/{}-embeddings-documents-w2i.pickle\".format(permission))\n",
    "        OwnDataCreator.save_document_based_dataset(infile, permission, embeddings, stemmer, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AcNetDataCreator:\n",
    "    def load_documents(infile, stemmer, embeddings):\n",
    "        print(\"Loading row {} \".format(infile))\n",
    "        # read training data\n",
    "        tagged_file = pd.read_csv(infile)\n",
    "        documents = []\n",
    "        acnet_map = {\n",
    "            \"RECORD_AUDIO\": \"MICROPHONE\",\n",
    "            \"READ_CONTACTS\": \"CONTACTS\",\n",
    "            \"READ_CALENDAR\": \"CALENDAR\",\n",
    "            \"ACCESS_FINE_LOCATION\": \"LOCATION\",\n",
    "            \"CAMERA\": \"CAMERA\",\n",
    "            \"READ_SMS\": \"SMS\",\n",
    "            \"READ_CALL_LOGS\": \"CALL_LOG\",\n",
    "            \"CALL_PHONE\": \"PHONE\",\n",
    "            \"WRITE_SETTINGS\": \"SETTINGS\",\n",
    "            \"GET_TASKS\": \"TASKS\",\n",
    "            \"STORAGE\": \"STORAGE\",\n",
    "        }\n",
    "\n",
    "        for idx, row in tagged_file.iterrows():\n",
    "            app_id = row[\"app_id\"]\n",
    "            sentence = row[\"sentence\"]\n",
    "\n",
    "            if documents == []:  # if it is the first document\n",
    "                documents.append(DocumentReport(app_id))\n",
    "            elif documents[-1].app_id != app_id:  # if it is a new document\n",
    "                documents.append(DocumentReport(app_id))\n",
    "\n",
    "            for permission in acnet_map:\n",
    "                if (\n",
    "                    permission not in documents[-1].permissions\n",
    "                    or row[acnet_map[permission]] == 1\n",
    "                ):\n",
    "                    documents[-1].permissions[permission] = row[acnet_map[permission]]\n",
    "\n",
    "            documents[-1].sentences.append(sentence)\n",
    "            preprocessed = NLPUtils.preprocess_sentence(sentence, stemmer)\n",
    "\n",
    "            filtered = [word for word in preprocessed if word in embeddings]\n",
    "            if filtered:\n",
    "                documents[-1].preprocessed_sentences.append(filtered)\n",
    "        print(\"Loading completed\")\n",
    "        return documents\n",
    "    \n",
    "    def load_sentences(infile, stemmer, embeddings):\n",
    "        print(\"Loading row {} \".format(infile))\n",
    "        # read training data\n",
    "        tagged_file = pd.read_csv(infile)\n",
    "        sentences = []\n",
    "        acnet_map = {\n",
    "            \"RECORD_AUDIO\": \"MICROPHONE\",\n",
    "            \"READ_CONTACTS\": \"CONTACTS\",\n",
    "            \"READ_CALENDAR\": \"CALENDAR\",\n",
    "            \"ACCESS_FINE_LOCATION\": \"LOCATION\",\n",
    "            \"CAMERA\": \"CAMERA\",\n",
    "            \"READ_SMS\": \"SMS\",\n",
    "            \"READ_CALL_LOGS\": \"CALL_LOG\",\n",
    "            \"CALL_PHONE\": \"PHONE\",\n",
    "            \"WRITE_SETTINGS\": \"SETTINGS\",\n",
    "            \"GET_TASKS\": \"TASKS\",\n",
    "            \"STORAGE\": \"STORAGE\",\n",
    "        }\n",
    "        for idx, row in tagged_file.iterrows():\n",
    "            app_id = row[\"app_id\"]\n",
    "            sentence = row[\"sentence\"]\n",
    "            sentence_report = SentenceReport(app_id, sentence)\n",
    "\n",
    "            for permission in acnet_map:\n",
    "                sentence_report.permissions[permission] = row[acnet_map[permission]]\n",
    "\n",
    "            preprocessed = NLPUtils.preprocess_sentence(sentence, stemmer)\n",
    "            sentence_report.preprocessed_sentence = [\n",
    "                word for word in preprocessed if word in embeddings\n",
    "            ]\n",
    "            if sentence_report.preprocessed_sentence != []:\n",
    "                sentences.append(sentence_report)\n",
    "        print(\"Loading completed\")\n",
    "        return sentences\n",
    "\n",
    "    def vocab(infile, stemmer, embeddings):\n",
    "        print(\"Loading row {} \".format(infile))\n",
    "        tagged_file = pd.read_csv(infile)\n",
    "        w2i = {}\n",
    "        for idx, row in tagged_file.iterrows():\n",
    "            sentence = row[\"sentence\"]\n",
    "            preprocessed = NLPUtils.preprocess_sentence(sentence, stemmer)\n",
    "            filtered = [word for word in preprocessed if word in embeddings]\n",
    "            for token in filtered:\n",
    "                if token not in w2i:\n",
    "                    w2i[token] = len(w2i)\n",
    "        print(\"Loading completed\")\n",
    "        return w2i\n",
    "\n",
    "    def filtered_vocab_embeddings(w2i, embeddings):\n",
    "        subset = {}\n",
    "        for key in w2i:\n",
    "            subset[key] = embeddings[key]\n",
    "        return subset\n",
    "\n",
    "    def save_sentence_based_dataset(infile, embeddings, stemmer, outfile):\n",
    "        sentences = AcNetDataCreator.load_sentences(infile, stemmer, embeddings)\n",
    "        w2i = AcNetDataCreator.vocab(infile, stemmer, embeddings)\n",
    "        subset_embeddings = AcNetDataCreator.filtered_vocab_embeddings(w2i, embeddings)\n",
    "        with open(outfile, \"wb\") as target:\n",
    "            pickle.dump([subset_embeddings, sentences, w2i], target)\n",
    "\n",
    "    def save_document_based_dataset(infile, embeddings, stemmer, outfile):\n",
    "        documents = AcNetDataCreator.load_documents(infile, stemmer, embeddings)\n",
    "        w2i = AcNetDataCreator.vocab(infile, stemmer, embeddings)\n",
    "        subset_embeddings = AcNetDataCreator.filtered_vocab_embeddings(w2i, embeddings)\n",
    "        with open(outfile, \"wb\") as target:\n",
    "            pickle.dump([subset_embeddings, documents, w2i], target)\n",
    "    \n",
    "    def run():\n",
    "        \"\"\"\n",
    "        For fasttext embeddings,\n",
    "        stemmer =  \"nostemmer\"\n",
    "        embeddings_file = os.path.join(os.environ[\"SECURITY_DATASETS\"], \"embeddings/cc.en.300.bin\")\n",
    "        embeddings, embeddings_dim = IOUtils.load_embeddings_file(embeddings_file, \"fasttext\", lower=True)\n",
    "        #sentences\n",
    "        outfile = os.path.join(os.environ[\"SECURITY_DATASETS\"], \"saved-parameters/saved-data/saved-data/ac-net/fasttext-embeddings-sentences-w2i.pickle\")\n",
    "        #documents\n",
    "        outfile = os.path.join(os.environ[\"SECURITY_DATASETS\"], \"saved-parameters/saved-data/saved-data/ac-net/fasttext-embeddings-documents-w2i.pickle\")\n",
    "\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        For no stem own embeddings,\n",
    "        stemmer =  \"nostemmer\"\n",
    "        embeddings_file = os.path.join(os.environ[\"SECURITY_DATASETS\"], \"embeddings/own-embeddings/sscraped_no_stemming_300.bin\")\n",
    "        embeddings, embeddings_dim = IOUtils.load_embeddings_file(embeddings_file, \"word2vec\", lower=True)\n",
    "        #sentences\n",
    "        outfile = os.path.join(os.environ[\"SECURITY_DATASETS\"], \"saved-parameters/saved-data/saved-data/ac-net/nostem-embeddings-sentences-w2i.pickle\")\n",
    "        #documents\n",
    "        outfile = os.path.join(os.environ[\"SECURITY_DATASETS\"], \"saved-parameters/saved-data/saved-data/ac-net/nostem-embeddings-documents-w2i.pickle\")\n",
    "\n",
    "        \"\"\"\n",
    "        stemmer = \"porter\" # \"nostemmer\"\n",
    "        embeddings_file = os.path.join(os.environ[\"SECURITY_DATASETS\"], \"embeddings/own-embeddings/scraped_with_porter_stemming_300.bin\")\n",
    "        embeddings, embeddings_dim = IOUtils.load_embeddings_file(embeddings_file, \"word2vec\", lower=True)\n",
    "        \"\"\"\n",
    "        #create sentence based embeddings\n",
    "        infile = os.path.join(os.environ[\"SECURITY_DATASETS\"], \"acnet-data/ACNET_DATASET.csv\")\n",
    "        outfile = os.path.join(os.environ[\"SECURITY_DATASETS\"],\"saved-parameters/saved-data/ac-net/embeddings-sentences-w2i.pickle\")\n",
    "        AcNetDataCreator.save_sentence_based_dataset(infile, embeddings, stemmer, outfile)\n",
    "        \"\"\"\n",
    "\n",
    "        #create document based embeddings\n",
    "        infile = os.path.join(os.environ[\"SECURITY_DATASETS\"], \"acnet-data/ACNET_DATASET.csv\")\n",
    "        outfile = os.path.join(os.environ[\"SECURITY_DATASETS\"],\"saved-parameters/saved-data/ac-net/embeddings-documents-w2i.pickle\")\n",
    "        AcNetDataCreator.save_document_based_dataset(infile, embeddings, stemmer, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading row /Users/huseyinalecakir/huseyin/Work/Security/datasets/created-data/STORAGE.csv \n",
      "Loading completed\n",
      "Loading row /Users/huseyinalecakir/huseyin/Work/Security/datasets/created-data/STORAGE.csv \n",
      "Loading completed\n",
      "Loading row /Users/huseyinalecakir/huseyin/Work/Security/datasets/created-data/RECORD_AUDIO.csv \n",
      "Loading completed\n",
      "Loading row /Users/huseyinalecakir/huseyin/Work/Security/datasets/created-data/RECORD_AUDIO.csv \n",
      "Loading completed\n",
      "Loading row /Users/huseyinalecakir/huseyin/Work/Security/datasets/created-data/READ_CONTACTS.csv \n",
      "Loading completed\n",
      "Loading row /Users/huseyinalecakir/huseyin/Work/Security/datasets/created-data/READ_CONTACTS.csv \n",
      "Loading completed\n"
     ]
    }
   ],
   "source": [
    "OwnDataCreator.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = \"porter\" # \"nostemmer\"\n",
    "embeddings_file = os.path.join(os.environ[\"SECURITY_DATASETS\"], \"embeddings/own-embeddings/scraped_with_porter_stemming_300.bin\")\n",
    "embeddings, embeddings_dim = IOUtils.load_embeddings_file(embeddings_file, \"word2vec\", lower=True)\n",
    "permission = \"STORAGE\"\n",
    "infile = os.path.join(os.environ[\"SECURITY_DATASETS\"], \"created-data/{}.csv\".format(permission))\n",
    "outfile = os.path.join(os.environ[\"SECURITY_DATASETS\"],\"saved-parameters/saved-data/own-data/{}-embeddings-documents-w2i.pickle\".format(permission))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading row /Users/huseyinalecakir/huseyin/Work/Security/datasets/created-data/STORAGE.csv \n",
      "Loading completed\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'DocumentReport' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-b4ff473a26f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOwnDataCreator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermission\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'DocumentReport' has no len()"
     ]
    }
   ],
   "source": [
    "documents = OwnDataCreator.load_documents(infile, permission, stemmer, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<common.DocumentReport object at 0x1a22095f98>\n",
      "<common.DocumentReport object at 0x1a2124feb8>\n",
      "<common.DocumentReport object at 0x1a21c85b70>\n",
      "<common.DocumentReport object at 0x1a221cd908>\n"
     ]
    }
   ],
   "source": [
    "for doc in documents:\n",
    "    if len(doc.preprocessed_sentences) == 0:\n",
    "        print(doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
