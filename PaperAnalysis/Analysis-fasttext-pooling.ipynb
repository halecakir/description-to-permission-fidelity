{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "\n",
    "import pickle\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "seed = 10\n",
    "\n",
    "import dynet_config\n",
    "\n",
    "# Declare GPU as the default device type\n",
    "dynet_config.set_gpu()\n",
    "# Set some parameters manualy\n",
    "dynet_config.set(mem=400, random_seed=seed)\n",
    "# Initialize dynet import using above configuration in the current scope\n",
    "import dynet as dy\n",
    "\n",
    "\n",
    "from utils.io_utils import IOUtils\n",
    "from utils.nlp_utils import NLPUtils\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self):\n",
    "        self.w2i = None\n",
    "        self.entries = None\n",
    "        self.train_entries = None\n",
    "        self.test_entries = None\n",
    "        self.ext_embedding = None\n",
    "        self.reviews = None\n",
    "        self.predicted_reviews = None\n",
    "\n",
    "    def to(self, device):\n",
    "        if self.entries:\n",
    "            for entry in self.entries:\n",
    "                entry.index_tensor = entry.index_tensor.to(device=device)\n",
    "        if self.reviews:\n",
    "            for doc_id in self.reviews:\n",
    "                for review in self.reviews[doc_id]:\n",
    "                    review.index_tensor = review.index_tensor.to(device=device)\n",
    "        if self.predicted_reviews:\n",
    "            for doc_id in self.predicted_reviews:\n",
    "                for review in self.predicted_reviews[doc_id]:\n",
    "                    review.index_tensor = review.index_tensor.to(device=device)\n",
    "\n",
    "    def load(self, infile):\n",
    "        with open(infile, \"rb\") as target:\n",
    "            self.ext_embeddings, self.entries, self.w2i = pickle.load(target)\n",
    "\n",
    "    def save_data(self, infile):\n",
    "        with open(infile, \"rb\") as target:\n",
    "            self.ext_embeddings, self.entries, self.w2i = pickle.dump(target)\n",
    "\n",
    "    def load_predicted_reviews(self, infile):\n",
    "        with open(infile, \"rb\") as target:\n",
    "            self.predicted_reviews = pickle.load(target)\n",
    "        for app_id in self.predicted_reviews.keys():\n",
    "            self.predicted_reviews[app_id].sort(\n",
    "                key=lambda x: x.prediction_result.item(), reverse=True\n",
    "            )\n",
    "\n",
    "    def load_reviews(self, infile):\n",
    "        with open(infile, \"rb\") as target:\n",
    "            self.reviews = pickle.load(target)\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, data, opt):\n",
    "        self.opt = opt\n",
    "        self.model = dy.ParameterCollection()\n",
    "        self.trainer = dy.MomentumSGDTrainer(self.model)\n",
    "        self.w2i = data.w2i\n",
    "        self.wdims = opt.embedding_size\n",
    "        self.ldims = opt.hidden_size\n",
    "        self.attsize = opt.attention_size\n",
    "\n",
    "        self.ext_embeddings = data.ext_embeddings\n",
    "        # Model Parameters\n",
    "        self.wlookup = self.model.add_lookup_parameters((len(self.w2i), self.wdims))\n",
    "\n",
    "        self.__load_external_embeddings()\n",
    "\n",
    "        if self.opt.encoder_dir == \"single\":\n",
    "            if self.opt.encoder_type == \"lstm\":\n",
    "                self.sentence_rnn = [\n",
    "                    dy.VanillaLSTMBuilder(1, self.wdims, self.ldims, self.model)\n",
    "                ]\n",
    "            elif self.opt.encoder_type == \"gru\":\n",
    "                self.sentence_rnn = [\n",
    "                    dy.GRUBuilder(1, self.wdims, self.ldims, self.model)\n",
    "                ]\n",
    "            self.attention_w = self.model.add_parameters((self.attsize, self.ldims))\n",
    "            self.attention_b = self.model.add_parameters(self.attsize)\n",
    "            self.att_context = self.model.add_parameters(self.attsize)\n",
    "            self.mlp_w = self.model.add_parameters((1, self.ldims + 2 * self.ldims))\n",
    "            self.mlp_b = self.model.add_parameters(1)\n",
    "        elif self.opt.encoder_dir == \"bidirectional\":\n",
    "            if self.opt.encoder_type == \"lstm\":\n",
    "                self.sentence_rnn = [\n",
    "                    dy.VanillaLSTMBuilder(1, self.wdims, self.ldims, self.model),\n",
    "                    dy.VanillaLSTMBuilder(1, self.wdims, self.ldims, self.model),\n",
    "                ]\n",
    "            elif self.opt.encoder_type == \"gru\":\n",
    "                self.sentence_rnn = [\n",
    "                    dy.GRUBuilder(1, self.wdims, self.ldims, self.model),\n",
    "                    dy.GRUBuilder(1, self.wdims, self.ldims, self.model),\n",
    "                ]\n",
    "\n",
    "            self.attention_w = self.model.add_parameters((self.attsize, 2 * self.ldims))\n",
    "            self.attention_b = self.model.add_parameters(self.attsize)\n",
    "            self.att_context = self.model.add_parameters(self.attsize)\n",
    "            self.mlp_w = self.model.add_parameters((1, 2 * self.ldims + 4 * self.ldims))\n",
    "            self.mlp_b = self.model.add_parameters(1)\n",
    "            \n",
    "\n",
    "    def __load_external_embeddings(self):\n",
    "        print(\"Initializing word embeddings by pre-trained vectors\")\n",
    "        count = 0\n",
    "        for word in self.w2i:\n",
    "            if word in self.ext_embeddings:\n",
    "                count += 1\n",
    "                self.wlookup.init_row(self.w2i[word], self.ext_embeddings[word])\n",
    "        print(\n",
    "            \"Vocab size: %d; #words having pretrained vectors: %d\"\n",
    "            % (len(self.w2i), count)\n",
    "        )\n",
    "\n",
    "    def save(self):\n",
    "        self.model.save(self.opt.model_checkpoint)\n",
    "\n",
    "    def load(self):\n",
    "        self.model.populate(self.opt.model_checkpoint)\n",
    "\n",
    "def write_file(filename, string):\n",
    "    with open(filename, \"a\") as target:\n",
    "        target.write(\"{}\\n\".format(string))\n",
    "        target.flush()\n",
    "\n",
    "\n",
    "def encode_sequence(model, seq, rnn_builder):\n",
    "    def predict_sequence(builder, inputs):\n",
    "        s_init = builder.initial_state()\n",
    "        return s_init.transduce(inputs)\n",
    "\n",
    "    if model.opt.encoder_dir == \"bidirectional\":\n",
    "        f_in = [entry for entry in seq]\n",
    "        b_in = [rentry for rentry in reversed(seq)]\n",
    "        forward_sequence = predict_sequence(rnn_builder[0], f_in)\n",
    "        backward_sequence = predict_sequence(rnn_builder[1], b_in)\n",
    "        return [\n",
    "            dy.concatenate([s1, s2])\n",
    "            for s1, s2 in zip(forward_sequence, backward_sequence)\n",
    "        ]\n",
    "    elif model.opt.encoder_dir == \"single\":\n",
    "        f_in = [entry for entry in seq]\n",
    "        state = rnn_builder[0].initial_state()\n",
    "        states = []\n",
    "        for entry in seq:\n",
    "            state = state.add_input(entry)\n",
    "            states.append(state.output())\n",
    "        return states\n",
    "\n",
    "\n",
    "def max_pooling(encoded_sequence):\n",
    "    values = np.array([encoding.value() for encoding in encoded_sequence])\n",
    "    min_indexes = np.argmax(values, axis=0)\n",
    "    pooled_context = dy.concatenate(\n",
    "        [encoded_sequence[row][col] for col, row in enumerate(min_indexes)]\n",
    "    )\n",
    "    return pooled_context\n",
    "\n",
    "\n",
    "def min_pooling(encoded_sequence):\n",
    "    values = np.array([encoding.value() for encoding in encoded_sequence])\n",
    "    min_indexes = np.argmin(values, axis=0)\n",
    "    pooled_context = dy.concatenate(\n",
    "        [encoded_sequence[row][col] for col, row in enumerate(min_indexes)]\n",
    "    )\n",
    "    return pooled_context\n",
    "\n",
    "\n",
    "def average_pooling(encoded_sequence):\n",
    "    averages = []\n",
    "    for col in range(encoded_sequence[0].dim()[0][0]):\n",
    "        avg = []\n",
    "        for row in range(len(encoded_sequence)):\n",
    "            avg.append(encoded_sequence[row][col])\n",
    "        averages.append(dy.average(avg))\n",
    "    return dy.concatenate(averages)\n",
    "\n",
    "\n",
    "def train_item(args, model, sentence):\n",
    "    loss = None\n",
    "    seq = [\n",
    "        model.wlookup[int(model.w2i.get(entry, 0))]\n",
    "        for entry in sentence.preprocessed_sentence\n",
    "    ]\n",
    "    if len(seq) > 0:\n",
    "        encoded_sequence = encode_sequence(model, seq, model.sentence_rnn)\n",
    "        global_max = max_pooling(encoded_sequence)\n",
    "        global_min = average_pooling(encoded_sequence)\n",
    "        if len(encoded_sequence) > 0:\n",
    "            att_mlp_outputs = []\n",
    "            for e in encoded_sequence:\n",
    "                mlp_out = (model.attention_w * e) + model.attention_b\n",
    "                att_mlp_outputs.append(mlp_out)\n",
    "\n",
    "            lst = []\n",
    "            for o in att_mlp_outputs:\n",
    "                lst.append(dy.exp(dy.sum_elems(dy.cmult(o, model.att_context))))\n",
    "\n",
    "            sum_all = dy.esum(lst)\n",
    "\n",
    "            probs = [dy.cdiv(e, sum_all) for e in lst]\n",
    "            att_context = dy.esum(\n",
    "                [dy.cmult(p, h) for p, h in zip(probs, encoded_sequence)]\n",
    "            )\n",
    "            context = dy.concatenate([att_context, global_max, global_min])\n",
    "            #context = dy.concatenate([att_context])\n",
    "            y_pred = dy.logistic((model.mlp_w * context) + model.mlp_b)\n",
    "\n",
    "            if sentence.permissions[args.permission_type]:\n",
    "                loss = dy.binary_log_loss(y_pred, dy.scalarInput(1))\n",
    "            else:\n",
    "                loss = dy.binary_log_loss(y_pred, dy.scalarInput(0))\n",
    "\n",
    "            loss.backward()\n",
    "            model.trainer.update()\n",
    "            loss_val = loss.scalar_value()\n",
    "            dy.renew_cg()\n",
    "            return loss_val\n",
    "    return 0\n",
    "\n",
    "\n",
    "def test_item(model, sentence):\n",
    "    seq = [\n",
    "        model.wlookup[int(model.w2i.get(entry, 0))]\n",
    "        for entry in sentence.preprocessed_sentence\n",
    "    ]\n",
    "    if len(seq) > 0:\n",
    "        encoded_sequence = encode_sequence(model, seq, model.sentence_rnn)\n",
    "        global_max = max_pooling(encoded_sequence)\n",
    "        global_min = average_pooling(encoded_sequence)\n",
    "        if len(encoded_sequence) > 0:\n",
    "            att_mlp_outputs = []\n",
    "            for e in encoded_sequence:\n",
    "                mlp_out = (model.attention_w * e) + model.attention_b\n",
    "                att_mlp_outputs.append(mlp_out)\n",
    "\n",
    "            lst = []\n",
    "            for o in att_mlp_outputs:\n",
    "                lst.append(dy.exp(dy.sum_elems(dy.cmult(o, model.att_context))))\n",
    "\n",
    "            sum_all = dy.esum(lst)\n",
    "\n",
    "            probs = [dy.cdiv(e, sum_all) for e in lst]\n",
    "            att_context = dy.esum(\n",
    "                [dy.cmult(p, h) for p, h in zip(probs, encoded_sequence)]\n",
    "            )\n",
    "            context = dy.concatenate([att_context, global_max, global_min])\n",
    "            #context = dy.concatenate([att_context])\n",
    "            y_pred = dy.logistic((model.mlp_w * context) + model.mlp_b)\n",
    "            sentence.prediction_result = y_pred.scalar_value()\n",
    "            dy.renew_cg()\n",
    "            return sentence.prediction_result\n",
    "    return 0\n",
    "\n",
    "def show_attention_weights(model, sentence):\n",
    "    seq = [\n",
    "        model.wlookup[int(model.w2i.get(entry, 0))]\n",
    "        for entry in sentence.preprocessed_sentence\n",
    "    ]\n",
    "    if len(seq) > 0:\n",
    "        encoded_sequence = encode_sequence(model, seq, model.sentence_rnn)\n",
    "        if len(encoded_sequence) > 0:\n",
    "            att_mlp_outputs = []\n",
    "            for e in encoded_sequence:\n",
    "                mlp_out = (model.attention_w * e) + model.attention_b\n",
    "                att_mlp_outputs.append(mlp_out)\n",
    "\n",
    "            lst = []\n",
    "            for o in att_mlp_outputs:\n",
    "                lst.append(dy.exp(dy.sum_elems(dy.cmult(o, model.att_context))))\n",
    "\n",
    "            sum_all = dy.esum(lst)\n",
    "            probs = [dy.cdiv(e, sum_all).scalar_value() for e in lst]\n",
    "            return probs\n",
    "\n",
    "\n",
    "def train_all(args, model, data):\n",
    "    write_file(args.outdir, \"Training...\")\n",
    "    losses = []\n",
    "    for index, sentence in enumerate(data.train_entries):\n",
    "        loss = train_item(args, model, sentence)\n",
    "        if index != 0:\n",
    "            if index % model.opt.print_every == 0:\n",
    "                write_file(\n",
    "                    args.outdir,\n",
    "                    \"Index {} Loss {}\".format(\n",
    "                        index, np.mean(losses[index - model.opt.print_every :])\n",
    "                    ),\n",
    "                )\n",
    "        losses.append(loss)\n",
    "\n",
    "\n",
    "def test_all(args, model, data):\n",
    "    def pr_roc_auc(predictions, gold):\n",
    "        y_true = np.array(gold)\n",
    "        y_scores = np.array(predictions)\n",
    "        roc_auc = roc_auc_score(y_true, y_scores)\n",
    "        pr_auc = average_precision_score(y_true, y_scores)\n",
    "        return roc_auc, pr_auc\n",
    "\n",
    "    write_file(args.outdir, \"Predicting..\")\n",
    "\n",
    "    predictions, gold = [], []\n",
    "    for index, sentence in enumerate(data.test_entries):\n",
    "        pred = test_item(model, sentence)\n",
    "        predictions.append(pred)\n",
    "        gold.append(sentence.permissions[args.permission_type])\n",
    "    return pr_roc_auc(predictions, gold)\n",
    "\n",
    "\n",
    "def kfold_validation(args, data):\n",
    "    data.entries = np.array(data.entries)\n",
    "    random.shuffle(data.entries)\n",
    "\n",
    "    kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "    roc_l, pr_l = [], []\n",
    "    for foldid, (train, test) in enumerate(kfold.split(data.entries)):\n",
    "        write_file(args.outdir, \"Fold {}\".format(foldid + 1))\n",
    "\n",
    "        model = Model(data, args)\n",
    "        data.train_entries = data.entries[train]\n",
    "        data.test_entries = data.entries[test]\n",
    "        max_roc_auc, max_pr_auc = 0, 0\n",
    "        for epoch in range(args.num_epoch):\n",
    "            train_all(args, model, data)\n",
    "            roc_auc, pr_auc = test_all(args, model, data)\n",
    "            if pr_auc > max_pr_auc:\n",
    "                max_pr_auc = pr_auc\n",
    "                max_roc_auc = roc_auc\n",
    "            write_file(\n",
    "                args.outdir, \"Epoch {} ROC {}  PR {}\".format(epoch + 1, roc_auc, pr_auc)\n",
    "            )\n",
    "        model.save()\n",
    "        write_file(args.outdir, \"ROC {} PR {}\".format(max_roc_auc, max_pr_auc))\n",
    "        roc_l.append(max_roc_auc)\n",
    "        pr_l.append(max_pr_auc)\n",
    "    write_file(\n",
    "        args.outdir, \"Summary : ROC {} PR {}\".format(np.mean(roc_l), np.mean(pr_l))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments:\n",
    "    permission_type = \"READ_CONTACTS\"\n",
    "    saved_data = \"/Users/huseyinalecakir/huseyin/Work/Security/datasets/saved-parameters/saved-data/ac-net/fasttext_embeddings-sentences-w2i.pickle\"\n",
    "    outdir = \"output.txt\"\n",
    "    stemmer = \"porter\"\n",
    "    embedding_size = 300\n",
    "    hidden_size = 128\n",
    "    attention_size = 128\n",
    "    output_size = 1\n",
    "    print_every = 1000\n",
    "    encoder_dir = \"bidirectional\"\n",
    "    encoder_type = \"gru\"\n",
    "    num_epoch = 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Arguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data()\n",
    "data.load(args.saved_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data.entries = np.array(data.entries)\\nrandom.shuffle(data.entries)\\nfrom sklearn.model_selection import train_test_split\\ndata.train_entries, data.test_entries = train_test_split(data.entries, test_size=0.10, random_state=5)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"data.entries = np.array(data.entries)\n",
    "random.shuffle(data.entries)\n",
    "from sklearn.model_selection import train_test_split\n",
    "data.train_entries, data.test_entries = train_test_split(data.entries, test_size=0.10, random_state=5)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_test(infile):\n",
    "    with open(infile, \"rb\") as target:\n",
    "        data.entries, data.train_entries, data.test_entries = pickle.load(target)\n",
    "\n",
    "def save_train_test(infile):\n",
    "    with open(infile, \"wb\") as target:\n",
    "        pickle.dump([data.entries, data.train_entries, data.test_entries], target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_train_test(\"fasttext_train_test.pickle\")\n",
    "load_train_test(\"fasttext_train_test.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing word embeddings by pre-trained vectors\n",
      "Vocab size: 18373; #words having pretrained vectors: 18373\n",
      "0.968143810694 0.708528414514\n"
     ]
    }
   ],
   "source": [
    "model = Model(data, args)\n",
    "train_all(args, model, data)\n",
    "roc_auc, pr_auc = test_all(args, model, data)\n",
    "print(roc_auc, pr_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in data.test_entries:\n",
    "    if type(entry.prediction_result) != float:\n",
    "        entry.prediction_result = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:67 - FN:20 - TN:2353 - FP:32\n",
      "Precision:0.6767676767676768 - Recall:0.7701149425287356\n",
      "Accuracy:0.9789644012944984\n",
      "F-measuse:0.7204301075268816\n"
     ]
    }
   ],
   "source": [
    "positives = [entry for entry in data.test_entries if entry.permissions[\"READ_CONTACTS\"]==1]\n",
    "negatives = [entry for entry in data.test_entries if entry.permissions[\"READ_CONTACTS\"]==0]\n",
    "\n",
    "sorted_positives = sorted(positives, key=lambda x: x.prediction_result, reverse=True)\n",
    "sorted_negatives = sorted(negatives, key=lambda x: x.prediction_result, reverse=True)\n",
    "threshold = 0.44 \n",
    "TP = sum([1 for entry in sorted_positives if entry.prediction_result >= threshold])\n",
    "FN = sum([1 for entry in sorted_positives if entry.prediction_result < threshold])\n",
    "TN = sum([1 for entry in sorted_negatives if entry.prediction_result < threshold])\n",
    "FP = sum([1 for entry in sorted_negatives if entry.prediction_result >= threshold])\n",
    "\n",
    "precision = TP/(TP+FP)\n",
    "recall = TP/(TP+FN)\n",
    "acc = (TN+TP)/(TN+FP+TP+FN)\n",
    "fmeasure = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "print(\"TP:{} - FN:{} - TN:{} - FP:{}\".format(TP, FN, TN, FP))\n",
    "print(\"Precision:{} - Recall:{}\".format(precision, recall))\n",
    "print(\"Accuracy:{}\".format(acc))\n",
    "print(\"F-measuse:{}\".format(fmeasure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"attention_pooling_fasttext_statement_sentences.txt\", \"w\") as target:\n",
    "    neg_keys = {}\n",
    "    for idx, entry in enumerate(sorted_negatives):\n",
    "        if entry.prediction_result > threshold:\n",
    "            weights = show_attention_weights(model, entry)\n",
    "            max_index = weights.index(max(weights))\n",
    "            mit = entry.preprocessed_sentence[max_index] #most_important_token\n",
    "            neg_keys[mit] = neg_keys[mit]+1 if mit in neg_keys else 1\n",
    "            pairs = [(t,w)for t,w in zip(entry.preprocessed_sentence,weights)]\n",
    "            target.write(\"{}::{}::{}\\n\".format(idx+1, entry.sentence, entry.prediction_result))\n",
    "            print(\"{}::{}::{}\\n\".format(idx+1, entry.sentence, entry.prediction_result))\n",
    "            for t,w in pairs:\n",
    "                print(\"{}:{}\".format(t,w))\n",
    "                target.write(\"{}:{}\\n\".format(t,w))\n",
    "            print()\n",
    "            target.write(\"\\n\")\n",
    "    sorted_lst = sorted(neg_keys.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    target.write(\"KEYWORDS\\n\")\n",
    "    for key, value in sorted_lst:\n",
    "        target.write(\"{}:{}\\n\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"attention_pooling_fasttext_permission_sentences.txt\", \"w\") as target:\n",
    "    pos_keys = {}\n",
    "    for idx, entry in enumerate(sorted_positives):\n",
    "        weights = show_attention_weights(model, entry)\n",
    "        max_index = weights.index(max(weights))\n",
    "        mit = entry.preprocessed_sentence[max_index] #most_important_token\n",
    "        pos_keys[mit] = pos_keys[mit]+1 if mit in pos_keys else 1\n",
    "        pairs = [(t,w)for t,w in zip(entry.preprocessed_sentence,weights)]\n",
    "        print(\"{}::{}::{}\\n\".format(idx+1, entry.sentence, entry.prediction_result))\n",
    "        target.write(\"{}::{}::{}\\n\".format(idx+1, entry.sentence, entry.prediction_result))\n",
    "        for t,w in pairs:\n",
    "            print(\"{}:{}\".format(t,w))\n",
    "            target.write(\"{}:{}\\n\".format(t,w))\n",
    "        print()\n",
    "        target.write(\"\\n\")\n",
    "    sorted_lst = sorted(pos_keys.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    target.write(\"KEYWORDS\\n\")\n",
    "    for key, value in sorted_lst:\n",
    "        target.write(\"{}:{}\\n\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cloud': 1, 'messages': 1, 'useful': 1, 'free': 1, 'sync': 1, 'want': 2, 'unwanted': 1, 'share': 2, 'friends': 2, 'save': 1, 'library': 1, 'buzz': 1, 'update': 1, 'backup': 1, 'like': 1, 'data': 1, 'connect': 1, 'number': 1, 'doodle': 1, 'high': 1, 'pay': 1, 'integration': 1, 'options': 1, 'dont': 1, 'app': 1, 'contact': 2, 'allows': 1, 'unique': 1}\n"
     ]
    }
   ],
   "source": [
    "print(neg_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'blacklist': 1, 'features': 1, 'merge': 2, 'need': 1, 'create': 1, 'text': 1, 'reads': 1, 'dial': 1, 'gallery': 1, 'backups': 1, 'nice': 1, 'block': 1, 'easily': 1, 'community': 1, 'app': 4, 'dont': 1, 'modes': 1, 'vips': 1, 'browse': 1, 'friends': 1, 'flexible': 1, 'history': 1, 'required': 2, 'supports': 1, 'pay': 4, 'media': 1, 'missed': 1, 'contact': 4, 'free': 2, 'c': 1, 'twitter': 1, 'types': 1, 'manage': 1, 'widget': 1, 'managed': 1, 'premium': 1, 'safety': 1, 'android': 2, 'sound': 1, 'may': 1, 'never': 1, 'much': 1, 'updates': 1, 'edit': 1, 'select': 1, 'found': 1, 'register': 1, 'anywhere': 1, 'use': 1, 'banking': 1, 'rate': 1, 'passwords': 1, 'around': 1, 'phone': 1, 'movie': 1, 'developers': 1, 'mail': 1, 'address': 2, 'effects': 1, 'schedule': 1, 'new': 1, 'application': 1, 'book': 1, 'gmail': 1, 'recall': 1, 'synchronize': 1, 'whitelist': 1, 'operate': 1, 'multiple': 1, 'avoid': 1, 'videos': 1, 'analyze': 1, 'choose': 1}\n"
     ]
    }
   ],
   "source": [
    "print(pos_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"test_entries = data.test_entries\n",
    "data.test_entries = data.train_entries\n",
    "roc_auc, pr_auc = test_all(args, model, data)\n",
    "\n",
    "ls = []\n",
    "ls.extend(data.test_entries)\n",
    "ls.extend(test_entries)\n",
    "false = sum([1 for entry in ls if entry.prediction_result  < 0.25 and entry.permissions[\"READ_CONTACTS\"] == 1 ])\n",
    "tagged = sum([1 for entry in ls if entry.permissions[\"READ_CONTACTS\"] == 1])\n",
    "print(false, tagged, (false/tagged))\n",
    "false = sum([1 for entry in ls if entry.prediction_result  < 0.25 and entry.permissions[\"READ_CONTACTS\"] == 1 ])\n",
    "tagged = sum([1 for entry in ls if entry.permissions[\"READ_CONTACTS\"] == 1])\n",
    "print(false, tagged, (false/tagged))\n",
    "for entry in ls:\n",
    "    print(idx+1, entry.sentence, entry.prediction_result)\n",
    "    print()\n",
    "false_positives = [entry for entry in sorted_negatives if entry.prediction_result >= 0.5]\n",
    "for entry in false_positives:\n",
    "    print(entry.prediction_result)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
