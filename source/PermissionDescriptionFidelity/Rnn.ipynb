{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dynet as dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_embeddings_file(file_name, embedding_type, lower=False):\n",
    "    if not os.path.isfile(file_name):\n",
    "        print(file_name, \"does not exist\")\n",
    "        return {}, 0\n",
    "        \n",
    "    if type == \"word2vec\":\n",
    "        model = KeyedVectors.load_word2vec_format(file_name, binary=True, unicode_errors=\"ignore\")\n",
    "        words = model.index2entity\n",
    "    elif type == \"fasttext\":\n",
    "        model = FastText.load_fasttext_format(file_name)\n",
    "        words = [w for w in model.wv.vocab]\n",
    "    else:\n",
    "        print(\"Unknown Type\")\n",
    "        return {}, 0\n",
    "\n",
    "    if lower:\n",
    "        vectors = {word.lower(): model[word] for word in words}\n",
    "    else:\n",
    "        vectors = {word: model[word] for word in words}\n",
    "\n",
    "    if \"UNK\" not in vectors:\n",
    "        unk = np.mean([vectors[word] for word in vectors.keys()], axis=0)\n",
    "        vectors[\"UNK\"] = unk\n",
    "\n",
    "    return vectors, len(vectors[\"UNK\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(sentences):\n",
    "    # normalize tha data and split data into sentences\n",
    "    # implement helper functions stopwords removal,lemmatization, remove named entities, bbbreviations\n",
    "    return []\n",
    "\n",
    "class Document:\n",
    "    def __init__(docId, docData, permList, w2i, perm2i):\n",
    "        self.id = docID\n",
    "        self.doc = docData # orginal doc string\n",
    "        sentences = preprocess(docData) # try another option: don't preprocess and just implement period handling\n",
    "        #convert sentences into  list of integers\n",
    "        #convert permissions into list of integers\n",
    "        #scores = Similiarity scores between permissions and list of sentences\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dynet as dy\n",
    "\n",
    "import random\n",
    "random.seed(33)\n",
    "\n",
    "class SimpleModel:\n",
    "    def __init__(documents, vocab, w2i, options):\n",
    "        self.model = ParameterCollection()\n",
    "        self.trainer = AdamTrainer(self.model)\n",
    "        \n",
    "        self.documents = documents \n",
    "        self.w2i = w2i\n",
    "        self.wdims = options.wembedding_dims\n",
    "        self.ldims = options.lstm_dims\n",
    "        \n",
    "        self.wlookup = self.model.add_lookup_parameters((len(w2i), self.wdims)) #PAD, and INITIAL tokens?\n",
    "        if options.external_embedding is not None:\n",
    "            ext_embeddings, ext_emb_dim = load_embeddings_file(options.external_embedding, lower=self.lowerCase, type=options.external_embedding_type)\n",
    "            assert (ext_emb_dim == self.wdims)\n",
    "            print(\"Initializing word embeddings by pre-trained vectors\")\n",
    "            count = 0\n",
    "            for word in self.vocab:\n",
    "                if word in ext_embeddings:\n",
    "                    count += 1\n",
    "                    self.wlookup.init_row(self.vocab[word], ext_embeddings[word])\n",
    "            self.ext_embeddings = ext_embeddings\n",
    "            print(\"Vocab size: %d; #words having pretrained vectors: %d\" % (len(self.vocab), count))\n",
    "            \n",
    "        self.sentence_rnn = [SimpleRNNBuilder(1, self.wdims, self.ldims, self.model)] # Try bi-rnn and lstm\n",
    "        self.permission_rrn = [SimpleRNNBuilder(1, self.wdims, self.ldims, self.model)] # Try bi-rnn and lstm\n",
    "    \n",
    "    def cos_similiariy(v1, v2):\n",
    "        from numpy import dot\n",
    "        from numpy.linalg import norm\n",
    "        return dot(a, b)/(norm(a)*norm(b))\n",
    "    \n",
    "    def train():\n",
    "        for doc in self.documents:\n",
    "            #TODO: gather all permission encodings and compare them with all sentences of the permission\n",
    "            rnn_forward = self.permission_rrn[0].initial_state()\n",
    "            perm_enc = None\n",
    "            for entry  in doc.permissions:\n",
    "                vec = self.wlookup[int(self.w2i.get(entry, 0))]\n",
    "                rnn_forward = lstm_forward.add_input(vec)\n",
    "                perm_enc = lstm_forward.output()\n",
    "            \n",
    "            #Sentence encoding\n",
    "            sentence_enc = None\n",
    "            for sentence in doc.sentences:\n",
    "                rnn_forward = self.sentence_rnn[0].initial_state()\n",
    "                for entry  in sentence:\n",
    "                    vec = self.wlookup[int(self.w2i.get(entry, 0))]\n",
    "                    rnn_forward = lstm_forward.add_input(vec)\n",
    "                    sentence_enc = lstm_forward.output()\n",
    "            #sample\n",
    "            perm_vec =  perm_enc.npvalue()\n",
    "            sentence_vec = sentence_enc.npvalue()\n",
    "            sim = cos_similiariy(perm_vec, sentence_vec) \n",
    "            \n",
    "            \n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
