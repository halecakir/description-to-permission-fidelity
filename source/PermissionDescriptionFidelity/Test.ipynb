{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import xlrd\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.wrappers import FastText\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "from log import logger\n",
    "from decorators import logging\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, doc_id, title, description, permissions, tags=None, raw_sentences=None):\n",
    "        self.id = doc_id\n",
    "        self.title = title\n",
    "        self.permissions = permissions\n",
    "        self.description = description \n",
    "        self.tags = tags\n",
    "        self.raw_sentences = raw_sentences\n",
    "\n",
    "    def __str__(self):\n",
    "        print(self.title)\n",
    "        print(self.permissions)\n",
    "\n",
    "\n",
    "\n",
    "class Permission:\n",
    "    def __init__(self, permission_type, permission_phrase):\n",
    "        self.ptype = permission_type\n",
    "        self.pphrase = permission_phrase\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Permission({}, {})\".format(self.ptype, \" \".join(self.pphrase))\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, Permission):\n",
    "            return ((self.ptype == other.ptype))\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.__repr__())\n",
    "\n",
    "\n",
    "class Utils:\n",
    "    CORE_NLP_DIR = r'/home/huseyin/LIB/stanford-corenlp-full-2018-10-05'\n",
    "    \n",
    "    @staticmethod\n",
    "    @logging\n",
    "    def preprocess(text):\n",
    "        paragrahps = text.split(\"\\n\")\n",
    "        sentences = []\n",
    "        for p in paragrahps:\n",
    "            for s in sent_tokenize(p):\n",
    "                sentences.append(s)\n",
    "        return sentences\n",
    "\n",
    "    @staticmethod\n",
    "    @logging\n",
    "    def remove_hyperlinks(text):\n",
    "        regex = r\"((https?:\\/\\/)?[^\\s]+\\.[^\\s]+)\"\n",
    "        text = re.sub(regex, '', text)\n",
    "        return text\n",
    "\n",
    "    @staticmethod\n",
    "    def to_lower(w, lower):\n",
    "        return w.lower() if lower else w\n",
    "    \n",
    "    @staticmethod\n",
    "    @logging\n",
    "    def vocab(file_path, file_type=\"csv\", lower=True):\n",
    "        nlp = StanfordCoreNLP(Utils.CORE_NLP_DIR)\n",
    "        wordsCount = Counter()\n",
    "        permissions = []\n",
    "        distincts_permissions = set()\n",
    "        if file_type == \"csv\":\n",
    "            with open(file_path) as f:\n",
    "                reader = csv.reader(f)\n",
    "                next(reader) # skip header\n",
    "                for row in reader:\n",
    "                    text = row[1]\n",
    "                    for sentence in Utils.preprocess(text):\n",
    "                        sentence = Utils.remove_hyperlinks(sentence)\n",
    "                        for w in nlp.word_tokenize(sentence):\n",
    "                            wordsCount.update([Utils.to_lower(w, lower)])\n",
    "                        for p in  row[2].strip().split(\",\"):\n",
    "                            ptype = Utils.to_lower(p, lower)\n",
    "                            if ptype not in distincts_permissions:\n",
    "                                pphrase = [Utils.to_lower(t, lower) for t in p.split(\"_\")]\n",
    "                                perm = Permission(ptype, pphrase)\n",
    "                                permissions.append(perm)\n",
    "                                distincts_permissions.add(ptype)\n",
    "                            for token in p.split(\"_\"):\n",
    "                                wordsCount.update([Utils.to_lower(token, lower)])\n",
    "        elif file_type == \"excel\":\n",
    "            handtagged_permissions = [\"READ_CALENDAR\", \"READ_CONTACTS\", \"RECORD_AUDIO\"]\n",
    "            loc = (file_path)\n",
    "            wb = xlrd.open_workbook(loc) \n",
    "            sheet = wb.sheet_by_index(0)\n",
    "            sharp_count = 0\n",
    "            apk_title = \"\"\n",
    "            for i in range(sheet.nrows):\n",
    "                sentence = sheet.cell_value(i, 0)\n",
    "                if sentence.startswith(\"##\"):\n",
    "                    sharp_count += 1\n",
    "                    if sharp_count % 2 == 1:\n",
    "                        apk_title = sentence.split(\"##\")[1] \n",
    "                else:\n",
    "                    if sharp_count != 0 and sharp_count % 2 == 0:\n",
    "                        sentence = sentence.strip()\n",
    "                        for w in nlp.word_tokenize(sentence):\n",
    "                            wordsCount.update([Utils.to_lower(w, lower)])\n",
    "                        for p in handtagged_permissions:\n",
    "                            ptype = Utils.to_lower(p, lower)\n",
    "                            if ptype not in distincts_permissions:\n",
    "                                pphrase = [Utils.to_lower(t, lower) for t in p.split(\"_\")]\n",
    "                                perm = Permission(ptype, pphrase)\n",
    "                                permissions.append(perm)\n",
    "                                distincts_permissions.add(ptype)\n",
    "                                for token in p.split(\"_\"):\n",
    "                                    wordsCount.update([Utils.to_lower(token, lower)])            \n",
    "        else:\n",
    "            raise Exception(\"Unsupported file type.\")\n",
    "        nlp.close()\n",
    "        return wordsCount.keys(), {w: i for i, w in enumerate(list(wordsCount.keys()))}, permissions\n",
    "\n",
    "    @staticmethod\n",
    "    @logging\n",
    "    def read_file(file_path, w2i, file_type=\"csv\", lower=True):\n",
    "        data = []\n",
    "        doc_id = 0\n",
    "        if file_type == \"csv\":\n",
    "            with open(file_path) as f:\n",
    "                reader = csv.reader(f)\n",
    "                next(reader) # skip header\n",
    "                for row in reader:\n",
    "                    doc_id += 1\n",
    "                    title = row[0]\n",
    "                    description = row[1]\n",
    "                    permissions = set()\n",
    "                    for p in  row[2].strip().split(\",\"):\n",
    "                        ptype = Utils.to_lower(p, lower)\n",
    "                        pphrase = [Utils.to_lower(t, lower) for t in p.split(\"_\")]\n",
    "                        perm = Permission(ptype, pphrase)\n",
    "                        permissions.add(perm)\n",
    "\n",
    "                    sentences = []\n",
    "                    raw_sentences = []\n",
    "                    for sentence in Utils.preprocess(description):\n",
    "                        sentence = Utils.remove_hyperlinks(sentence)\n",
    "                        sentences.append(sentence.strip())\n",
    "                        raw_sentences.append(sentence.strip())\n",
    "                        ###sentences.append([Utils.to_lower(w, lower) for w in word_tokenize(sentence)])           \n",
    "                    yield Document(doc_id, title, sentences, permissions,raw_sentences=raw_sentences)\n",
    "                    \n",
    "        elif file_type == \"excel\":\n",
    "            permission_title = file_path.split(\"/\")[-1].split(\".\")[0]\n",
    "            loc = (file_path)\n",
    "            wb = xlrd.open_workbook(loc) \n",
    "            sheet = wb.sheet_by_index(0)\n",
    "            sharp_count = 0\n",
    "            title = \"\"\n",
    "            permissions = set()\n",
    "            sentences = []\n",
    "            raw_sentences = []\n",
    "            tags = []\n",
    "            for i in range(sheet.nrows):\n",
    "                sentence = sheet.cell_value(i, 0)\n",
    "                if sentence.startswith(\"##\"):\n",
    "                    sharp_count += 1\n",
    "                    if sharp_count % 2 == 1:\n",
    "                        if doc_id > 0:\n",
    "                            yield Document(doc_id, title, sentences, permissions, tags, raw_sentences)\n",
    "                        \n",
    "                        #Document init values\n",
    "                        title = sentence.split(\"##\")[1]\n",
    "                        permissions = set()\n",
    "                        sentences = []\n",
    "                        raw_sentences = []\n",
    "                        tags = []\n",
    "                        doc_id += 1\n",
    "                        \n",
    "                        # Permissions for apk\n",
    "                        ptype = Utils.to_lower(permission_title, lower)\n",
    "                        pphrase = [Utils.to_lower(t, lower) for t in permission_title.split(\"_\")]\n",
    "                        perm = Permission(ptype, pphrase)\n",
    "                        permissions.add(perm)\n",
    "                else:\n",
    "                    if sharp_count != 0 and sharp_count % 2 == 0:\n",
    "                        sentences.append(sentence.strip())\n",
    "                        raw_sentences.append(sentence.strip())\n",
    "                        ###sentences.append([Utils.to_lower(w, lower) for w in word_tokenize(sentence.strip())]) \n",
    "                        tags.append(int(sheet.cell_value(i, 1)))\n",
    "                        \n",
    "            yield Document(doc_id, title, sentences, permissions, tags, raw_sentences)\n",
    "            wb.release_resources()\n",
    "            del wb\n",
    "        else:\n",
    "            raise Exception(\"Unsupported file type.\")\n",
    "    \n",
    "    @staticmethod\n",
    "    @logging\n",
    "    def get_data(file_path, w2i, sequence_type=\"dependency\", file_type=\"csv\", window_size=2, lower=True):\n",
    "        nlp = StanfordCoreNLP(Utils.CORE_NLP_DIR)\n",
    "        if sequence_type == \"raw\":\n",
    "            return Utils.read_file_raw(file_path, w2i, nlp, file_type, lower)\n",
    "        elif sequence_type == \"dependency\":\n",
    "            return Utils.read_file_dependency(file_path, w2i, nlp, file_type, lower)\n",
    "        elif sequence_type == \"windowed\":\n",
    "            return Utils.read_file_window(file_path, w2i, nlp, file_type, window_size, lower)\n",
    "        else:\n",
    "            nlp.close()\n",
    "            raise Exception(\"Unknown sequence type\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def read_file_raw(file_path, w2i, nlp, file_type=\"csv\", lower=True):\n",
    "        for doc in Utils.read_file(file_path, w2i, file_type, lower):\n",
    "            doc.description = [[Utils.to_lower(w, lower) for w in nlp.word_tokenize(sentence)] for sentence in doc.description]\n",
    "            yield doc\n",
    "        \n",
    "    @staticmethod\n",
    "    def read_file_window(file_path, w2i, nlp, file_type=\"csv\", window_size=2, lower=True):\n",
    "        for doc in Utils.read_file(file_path, w2i, file_type, lower):\n",
    "            doc.description = [[Utils.to_lower(w, lower) for w in nlp.word_tokenize(sentence)] for sentence in doc.description]\n",
    "            doc.description = Utils.split_into_windows(doc.description, window_size)\n",
    "            yield doc\n",
    "            \n",
    "    @staticmethod\n",
    "    def read_file_dependency(file_path, w2i, nlp, file_type=\"csv\", lower=True):\n",
    "        for doc in Utils.read_file(file_path, w2i, file_type, lower):\n",
    "            doc.description = Utils.split_into_dependencies(doc.description, nlp)\n",
    "            yield doc\n",
    "            \n",
    "    @staticmethod\n",
    "    def split_into_dependencies(sentences, nlp):\n",
    "        splitted_sentences = []\n",
    "        for sentence in sentences:\n",
    "            tokens = nlp.word_tokenize(sentence)\n",
    "            s = [[tokens[rel[1]-1], tokens[rel[2]-1]] for rel in nlp.dependency_parse(sentence) if rel[0] != 'ROOT']\n",
    "            splitted_sentences.append(s)\n",
    "        return splitted_sentences\n",
    "    \n",
    "    @staticmethod\n",
    "    def split_into_windows(sentences, window_size=2):\n",
    "        splitted_sentences = []\n",
    "        for sentence in sentences:\n",
    "            splitted_sentences.append([])\n",
    "            if len(sentence) < window_size:\n",
    "                splitted_sentences[-1].append(sentence)\n",
    "            else:\n",
    "                for start in range(len(sentence) - window_size + 1):\n",
    "                    splitted_sentences[-1].append([sentence[i+start] for i in range(window_size)])\n",
    "        return splitted_sentences\n",
    "    \n",
    "    @staticmethod     \n",
    "    def load_embeddings_file(file_name, embedding_type, lower=True):\n",
    "        if not os.path.isfile(file_name):\n",
    "            raise Exception(\"{} does not exist\".format(file_name))\n",
    "            \n",
    "        if embedding_type == \"word2vec\":\n",
    "            model = KeyedVectors.load_word2vec_format(file_name, binary=True, unicode_errors=\"ignore\")\n",
    "            words = model.index2entity\n",
    "        elif embedding_type == \"fasttext\":\n",
    "            model = FastText.load_fasttext_format(file_name)\n",
    "            words = [w for w in model.wv.vocab]\n",
    "        elif embedding_type == \"pickle\":\n",
    "            with open(file_name,'rb') as fp:\n",
    "                model = pickle.load(fp)\n",
    "                words = model.keys()\n",
    "        else:\n",
    "            raise Exception(\"Unknown Type\")\n",
    "\n",
    "        if lower:\n",
    "            vectors = {word.lower(): model[word] for word in words}\n",
    "        else:\n",
    "            vectors = {word: model[word] for word in words}\n",
    "\n",
    "        if \"UNK\" not in vectors:\n",
    "            unk = np.mean([vectors[word] for word in vectors.keys()], axis=0)\n",
    "            vectors[\"UNK\"] = unk\n",
    "\n",
    "        return vectors, len(vectors[\"UNK\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptionParser:\n",
    "    def __init__(self, train, train_file_type, external_embedding, external_embedding_type, wembedding_dims, lstm_dims):\n",
    "        self.train = train\n",
    "        self.train_file_type = train_file_type\n",
    "        self.external_embedding = external_embedding\n",
    "        self.external_embedding_type = external_embedding_type\n",
    "        self.wembedding_dims = wembedding_dims\n",
    "        self.lstm_dims = lstm_dims\n",
    "    \n",
    "options = OptionParser(\"/home/huseyin/Desktop/Security/data/whyper/Read_Calendar.xls\",\n",
    "                       \"excel\",\n",
    "                       \"/home/huseyin/Desktop/Security/data/subset_fasttext.bin\",\n",
    "                       \"pickle\",\n",
    "                       300,\n",
    "                       128)\n",
    "\n",
    "\n",
    "ext_embeddings_arg, ext_emb_dim_arg = Utils.load_embeddings_file(options.external_embedding, options.external_embedding_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dynet_config\n",
    "# Declare GPU as the default device type\n",
    "dynet_config.set_gpu()\n",
    "# Set some parameters manualy\n",
    "dynet_config.set(mem=400,random_seed=9)\n",
    "# Initialize dynet import using above configuration in the current scope\n",
    "\n",
    "import dynet as dy\n",
    "\n",
    "from numpy import inf\n",
    "import random\n",
    "random.seed(33)\n",
    "\n",
    "class SimpleModel:\n",
    "    def __init__(self, vocab, w2i, permissions, options):\n",
    "        self.model = dy.ParameterCollection()\n",
    "        self.trainer = dy.AdamTrainer(self.model)\n",
    "        \n",
    "        self.w2i = w2i\n",
    "        self.i2w = {w2i[w]:w for w in w2i}\n",
    "        self.wdims = options.wembedding_dims\n",
    "        self.ldims = options.lstm_dims\n",
    "        self.all_permissions = permissions\n",
    "        self.train_file_type = options.train_file_type\n",
    "        \n",
    "        self.wlookup = self.model.add_lookup_parameters((len(w2i), self.wdims)) #PAD, and INITIAL tokens?\n",
    "        if options.external_embedding is not None:\n",
    "            ext_embeddings, ext_emb_dim =  Utils.load_embeddings_file(options.external_embedding, options.external_embedding_type)\n",
    "            assert (ext_emb_dim == self.wdims)\n",
    "            print(\"Initializing word embeddings by pre-trained vectors\")\n",
    "            count = 0\n",
    "            for word in self.w2i:\n",
    "                if word in ext_embeddings:\n",
    "                    count += 1\n",
    "                    self.wlookup.init_row(self.w2i[word], ext_embeddings[word])\n",
    "            self.ext_embeddings = ext_embeddings\n",
    "            print(\"Vocab size: %d; #words having pretrained vectors: %d\" % (len(self.w2i), count))\n",
    "            \n",
    "        self.sentence_rnn = [dy.SimpleRNNBuilder(1, self.wdims, self.ldims, self.model)] # Try bi-rnn and lstm\n",
    "        self.permission_rrn = [dy.SimpleRNNBuilder(1, self.wdims, self.ldims, self.model)] # Try bi-rnn and lstm\n",
    "    \n",
    "    def cos_similiariy(self, v1, v2):\n",
    "        from numpy import dot\n",
    "        from numpy.linalg import norm\n",
    "        return dot(v1, v2)/(norm(v1)*norm(v2))\n",
    "    \n",
    "    def cosine_proximity(self, pred, gold):\n",
    "        def l2_normalize(x):\n",
    "            square_sum = dy.sqrt(dy.bmax(dy.sum_elems(dy.square(x)), np.finfo(float).eps * dy.ones((1))[0]))\n",
    "            return dy.cdiv(x, square_sum)\n",
    "\n",
    "        y_true = l2_normalize(pred)\n",
    "        y_pred = l2_normalize(gold)\n",
    "        return -dy.sum_elems(dy.cmult(y_true, y_pred))\n",
    "    \n",
    "    def cosine_loss(self, pred, gold):\n",
    "        sn1 = dy.l2_norm(pred)\n",
    "        sn2 = dy.l2_norm(gold)\n",
    "        mult = dy.cmult(sn1,sn2)\n",
    "        dot = dy.dot_product(pred,gold)\n",
    "        div = dy.cdiv(dot,mult)\n",
    "        y = dy.scalarInput(2)\n",
    "        res = dy.cdiv(1-div,y)\n",
    "        return res\n",
    "    \n",
    "    def sentence_permission_sim(self, sentences, perm):\n",
    "        max_sim = -inf\n",
    "        max_index = 0\n",
    "        for index, sentence_enc in enumerate(sentences):\n",
    "            sim = self.cos_similiariy(sentence_enc, perm)\n",
    "            if max_sim < sim: \n",
    "                max_sim = sim\n",
    "                max_index = index\n",
    "        return max_sim, max_index\n",
    "\n",
    "    def statistics(self, similarities):\n",
    "        statistics = {}\n",
    "        for app_id in similarities.keys():\n",
    "            statistics[app_id] = {\"related\": { \"all\" : []},\n",
    "                                  \"unrelated\": {\"all\" : []}}\n",
    "            max_related, max_unrelated = -inf, -inf\n",
    "            avg_related, avg_unrelated = 0, 0\n",
    "            for related_p in similarities[app_id][\"related\"]:\n",
    "                statistics[app_id][\"related\"][\"all\"].append(related_p[1])\n",
    "\n",
    "            for unrelated_p in similarities[app_id][\"unrelated\"]:\n",
    "                statistics[app_id][\"unrelated\"][\"all\"].append(unrelated_p[1])\n",
    "        return statistics\n",
    "    \n",
    "    @logging\n",
    "    def train(self, documents):\n",
    "        tagged_loss = 0\n",
    "        untagged_loss = 0 \n",
    "        for doc in documents:\n",
    "            if doc.description:                \n",
    "                #Sentence encoding\n",
    "                sentence_enc_s = []\n",
    "                for sentence,tag in zip(doc.description, doc.tags):\n",
    "                    sentence_enc_s.append([])\n",
    "                    if tag == 1 or tag == 2 or tag == 3:\n",
    "                        for window in sentence:\n",
    "                            permission_vecs = {}\n",
    "                            # gather all permission encoding of permissions\n",
    "                            for perm  in self.all_permissions:\n",
    "                                rnn_forward = self.permission_rrn[0].initial_state()\n",
    "                                for entry in perm.pphrase:\n",
    "                                    vec = self.wlookup[int(self.w2i.get(entry, 0))]\n",
    "                                    rnn_forward = rnn_forward.add_input(vec)\n",
    "                                permission_vecs[perm.ptype] = rnn_forward.output()\n",
    "                                \n",
    "                            rnn_forward = self.sentence_rnn[0].initial_state()\n",
    "                            for entry in window:\n",
    "                                vec = self.wlookup[int(self.w2i.get(entry, 0))]\n",
    "                                rnn_forward = rnn_forward.add_input(vec)\n",
    "                            loss = []\n",
    "                            total_e = 0\n",
    "                            for perm in self.all_permissions:\n",
    "                                if perm in doc.permissions:\n",
    "                                    e = self.cosine_loss(rnn_forward.output(), permission_vecs[perm.ptype])\n",
    "                                    loss.append(1-e)\n",
    "                                else:\n",
    "                                    e = self.cosine_loss(rnn_forward.output(), permission_vecs[perm.ptype])\n",
    "                                    loss.append(e)\n",
    "                            loss = dy.esum(loss)\n",
    "                            tagged_loss += loss.scalar_value()\n",
    "                            loss.backward()\n",
    "                            self.trainer.update()\n",
    "                            dy.renew_cg()\n",
    "                    \n",
    "                    elif tag == 0:\n",
    "                        for window in sentence:\n",
    "                            permission_vecs = {}\n",
    "                            # gather all permission encoding of permissions\n",
    "                            for perm  in self.all_permissions:\n",
    "                                rnn_forward = self.permission_rrn[0].initial_state()\n",
    "                                for entry in perm.pphrase:\n",
    "                                    vec = self.wlookup[int(self.w2i.get(entry, 0))]\n",
    "                                    rnn_forward = rnn_forward.add_input(vec)\n",
    "                                permission_vecs[perm.ptype] = rnn_forward.output()\n",
    "                                \n",
    "                            rnn_forward = self.sentence_rnn[0].initial_state()\n",
    "                            for entry in window:\n",
    "                                vec = self.wlookup[int(self.w2i.get(entry, 0))]\n",
    "                                rnn_forward = rnn_forward.add_input(vec)\n",
    "                            loss = []\n",
    "                            for perm in doc.permissions:\n",
    "                                loss.append(self.cosine_loss(rnn_forward.output(), permission_vecs[perm.ptype]))\n",
    "                            loss = dy.esum(loss)\n",
    "                            untagged_loss += loss.scalar_value()\n",
    "                            loss.backward()\n",
    "                            self.trainer.update()\n",
    "                            dy.renew_cg()\n",
    "        print(\"Total loss : {} - Tagged Loss {} - Untagged loss {}\".format(tagged_loss+untagged_loss, tagged_loss, untagged_loss))\n",
    "                            \n",
    "    @logging\n",
    "    def test(self, documents, print_mode=False):\n",
    "        document_permission_similiarities = {}\n",
    "        permission_vecs = {}\n",
    "        # gather all permission encoding of permissions\n",
    "        for perm  in self.all_permissions:\n",
    "            rnn_forward = self.permission_rrn[0].initial_state()\n",
    "            for entry in perm.pphrase:\n",
    "                vec = self.wlookup[int(self.w2i.get(entry, 0))]\n",
    "                rnn_forward = rnn_forward.add_input(vec)\n",
    "            permission_vecs[perm.ptype] = rnn_forward.output().npvalue()\n",
    "        for doc in documents:\n",
    "            if doc.description:   \n",
    "                document_permission_similiarities[doc.id] = {\"related\": [], \"unrelated\" : []}\n",
    "                if print_mode:\n",
    "                    print(\"\\n\\nDocument {}\".format(doc.id))\n",
    "                    for sent_id,sent in enumerate(doc.raw_sentences):\n",
    "                        print(\"Sentence ID {} : {}\".format(sent_id + 1, sent))\n",
    "\n",
    "                for sent_id, (sentence, tag) in enumerate(zip(doc.description, doc.tags)):\n",
    "                    sentence_enc_s = []\n",
    "                    if tag == 1 or tag == 2 or tag == 3:\n",
    "                        for window in sentence:\n",
    "                            rnn_forward = self.sentence_rnn[0].initial_state()\n",
    "                            for entry in window:\n",
    "                                vec = self.wlookup[int(self.w2i.get(entry, 0))]\n",
    "                                rnn_forward = rnn_forward.add_input(vec)\n",
    "                            if rnn_forward.output() is not None:\n",
    "                                rnn_forward.output().npvalue()\n",
    "                                sentence_enc_s.append(rnn_forward.output().npvalue())\n",
    "                            dy.renew_cg()\n",
    "\n",
    "                        for perm in self.all_permissions:\n",
    "                            max_sim, max_index = self.sentence_permission_sim(sentence_enc_s, permission_vecs[perm.ptype])\n",
    "                            if perm in doc.permissions:\n",
    "                                document_permission_similiarities[doc.id][\"related\"].append((perm.ptype, max_sim))\n",
    "                                if print_mode:\n",
    "                                    print(\"Sentence ID {} Dependency {}\".format(doc.id, sent_id+1, sentence[max_index]))\n",
    "                            else:\n",
    "                                document_permission_similiarities[doc.id][\"unrelated\"].append((perm.ptype, max_sim))\n",
    "        return document_permission_similiarities\n",
    "    \n",
    "    @logging\n",
    "    def train_test_split(self, file_path, window_size=2):\n",
    "        documents = []\n",
    "        for doc in Utils.get_data(file_path,\n",
    "                                self.w2i, \n",
    "                                sequence_type=\"windowed\", \n",
    "                                file_type=self.train_file_type, \n",
    "                                window_size=window_size, \n",
    "                                lower=True):\n",
    "            documents.append(doc)\n",
    "        random.shuffle(documents)\n",
    "        split_point = (3*len(documents))//4\n",
    "        return documents[:split_point], documents[split_point:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.36 s, sys: 344 ms, total: 4.7 s\n",
      "Wall time: 8.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "words, w2i, permissions = Utils.vocab(options.train, file_type=options.train_file_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing word embeddings by pre-trained vectors\n",
      "Vocab size: 4834; #words having pretrained vectors: 4289\n",
      "CPU times: user 112 ms, sys: 0 ns, total: 112 ms\n",
      "Wall time: 111 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = SimpleModel(words, w2i, permissions, options, ext_embeddings_arg, ext_emb_dim_arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "> <ipython-input-3-d4eb11094994>(138)train()\n",
      "-> e = self.cosine_loss(rnn_forward.output(), permission_vecs[perm.ptype])\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1  [ 0.28197637  0.26836446 -0.5771116   0.29709083 -0.06676818  0.12649925\n",
      " -0.46922001 -0.02415132 -0.11942988  0.06959423  0.39799285  0.07138018\n",
      "  0.34649134  0.3245213   0.40272585  0.56679249 -0.20529702  0.26524231\n",
      " -0.12629075 -0.21554518  0.13492306  0.2466245   0.59949291  0.18071526\n",
      " -0.00242077 -0.22546658  0.1052971   0.03577831  0.38247961 -0.1364498\n",
      "  0.07308594 -0.06833828 -0.17871222  0.25737375 -0.43688616  0.2661911\n",
      " -0.00721624 -0.02983778  0.10380994 -0.57350576 -0.12251846 -0.25539702\n",
      "  0.00701244 -0.21114126 -0.3260105  -0.32533237  0.03239485 -0.04423056\n",
      " -0.3075071  -0.10436306 -0.11462587  0.44353887  0.00139291  0.27557957\n",
      " -0.28767073  0.32065353 -0.32273692 -0.18010016 -0.09110064  0.20939957\n",
      "  0.23760918  0.13514437 -0.0539509  -0.08581656 -0.19230121 -0.32899281\n",
      "  0.07909564 -0.1419896   0.0435673  -0.38266981 -0.00761344  0.44551566\n",
      " -0.11568639  0.21034892 -0.33972254  0.16687268 -0.14262739  0.25385362\n",
      "  0.11944853  0.49373591 -0.32141697 -0.43412757 -0.00899512 -0.06247959\n",
      "  0.40312991  0.08232001 -0.24573828  0.09013318 -0.05132697 -0.25301099\n",
      "  0.1015451   0.15544747  0.18680806 -0.06211306 -0.1335384  -0.08497082\n",
      "  0.24081011  0.10677689 -0.37148055  0.12125511  0.100802    0.23660852\n",
      " -0.1347678   0.25190499  0.21966586 -0.09433431 -0.55810207 -0.02881126\n",
      "  0.19801463 -0.21764119  0.15978673 -0.46972588  0.21425608 -0.02549784\n",
      "  0.28651062  0.51841849  0.26477975 -0.14364979  0.3691504  -0.37762904\n",
      " -0.32353592 -0.14254621 -0.10293508  0.0495198   0.12102648 -0.39522931\n",
      "  0.26052743  0.52809858]\n",
      "v2  [-0.09284541  0.0476921  -0.03074134 -0.09792247  0.06505188 -0.14048213\n",
      " -0.11698982  0.01938094  0.04129727 -0.13510202 -0.17370336 -0.19955802\n",
      "  0.0317992  -0.03063762 -0.15565477  0.15695414 -0.05999379 -0.09935006\n",
      " -0.15024383  0.11223012  0.18808818 -0.16078323 -0.11507499 -0.07396308\n",
      "  0.02268023 -0.0318894  -0.10863571 -0.01945386  0.05294104  0.03398672\n",
      "  0.12150874 -0.17422137 -0.004287   -0.00804251 -0.01413499 -0.16214029\n",
      "  0.09976169 -0.18601084 -0.05714051  0.00641241 -0.00822508 -0.20561087\n",
      "  0.15464908  0.01297706 -0.11753105  0.18985094  0.11859442 -0.08836234\n",
      " -0.01691117 -0.05582882  0.02773227  0.00989585  0.05608159 -0.05651541\n",
      "  0.11140725  0.02156805  0.00993489 -0.01510524  0.12509279  0.02254112\n",
      " -0.08390507 -0.041434    0.04037011 -0.08850395 -0.02023637  0.17468126\n",
      " -0.00090118  0.05994922 -0.07686507 -0.12685911 -0.01494991 -0.26228225\n",
      " -0.12703843  0.18744084  0.06582572  0.12214763 -0.01116346  0.03096372\n",
      "  0.04991386  0.11263306 -0.03005199  0.09856684 -0.09872415  0.07803406\n",
      "  0.11427178  0.01493421  0.01009264  0.03645252  0.06803697 -0.13125904\n",
      " -0.07821041 -0.0602841   0.02313385  0.0450722   0.02565379 -0.07527315\n",
      " -0.06495117 -0.10326228 -0.03043487 -0.01674961 -0.24044476  0.05491086\n",
      " -0.04534304 -0.05142279 -0.10667333 -0.18114826  0.01500797 -0.14359652\n",
      " -0.04278738 -0.00249775 -0.23412751  0.13283804 -0.08174159 -0.09403585\n",
      "  0.02200903 -0.04348668  0.04262321 -0.11284587  0.00847497  0.01501882\n",
      "  0.14941871  0.1904445  -0.08131282  0.17921652 -0.08442499 -0.0894489\n",
      " -0.21286862  0.05938676]\n",
      "cosine dy  [0.55921185]\n",
      "cosine np  0.5592118452539021\n",
      "cosine ln  0.5592118452539021\n",
      "> <ipython-input-3-d4eb11094994>(139)train()\n",
      "-> print(e.scalar_value())\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5592118501663208\n",
      "> <ipython-input-3-d4eb11094994>(140)train()\n",
      "-> loss.append(1-e)\n"
     ]
    }
   ],
   "source": [
    "@logging\n",
    "def draw_histogram(data, img_name):\n",
    "    stats = model.statistics(data)   \n",
    "    related_all = []\n",
    "    unrelated_all = []\n",
    "    for doc_id in stats:\n",
    "        related_all.extend([i for i in stats[doc_id][\"related\"][\"all\"] if i > -inf])\n",
    "        unrelated_all.extend([i for i in stats[doc_id][\"unrelated\"][\"all\"] if i > -inf])\n",
    "        \n",
    "    from matplotlib import pyplot\n",
    "\n",
    "    pyplot.title(\"All similarity\")\n",
    "    pyplot.hist(related_all, bins='auto', alpha=0.5, label='related')\n",
    "    pyplot.hist(unrelated_all, bins='auto', alpha=0.5, label='unrelated')\n",
    "    pyplot.legend(loc='upper right')\n",
    "    pyplot.savefig(img_name)\n",
    "    pyplot.clf()\n",
    "\n",
    "train_data, test_data = model.train_test_split(options.train)\n",
    "similarities = model.test(test_data)\n",
    "draw_histogram(similarities, \"trained_epoch_{}.png\".format(0))\n",
    "model.test(test_data)\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"Epoch {}\".format(i+1))\n",
    "    model.train(train_data)\n",
    "    similarities = model.test(test_data)\n",
    "    draw_histogram(similarities, \"trained_epoch_{}.png\".format(i+1))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
