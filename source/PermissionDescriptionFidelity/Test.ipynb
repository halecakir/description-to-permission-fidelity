{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import xlrd\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.wrappers import FastText\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "from log import logger\n",
    "from decorators import logging\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, doc_id, title, description, permissions, tags=None):\n",
    "        self.id = doc_id\n",
    "        self.title = title\n",
    "        self.permissions = permissions\n",
    "        self.description = description \n",
    "        self.tags = tags\n",
    "\n",
    "    def __str__(self):\n",
    "        print(self.title)\n",
    "        print(self.permissions)\n",
    "\n",
    "\n",
    "class Permission:\n",
    "    def __init__(self, permission_type, permission_phrase):\n",
    "        self.ptype = permission_type\n",
    "        self.pphrase = permission_phrase\n",
    "    \n",
    "    def __str__(self):\n",
    "        print(self.ptype)\n",
    "        print(self.pphrase)\n",
    "\n",
    "\n",
    "class Utils:\n",
    "    CORE_NLP_DIR = r'/home/huseyin/LIB/stanford-corenlp-full-2018-10-05'\n",
    "    \n",
    "    @staticmethod\n",
    "    @logging\n",
    "    def preprocess(text):\n",
    "        paragrahps = text.split(\"\\n\")\n",
    "        sentences = []\n",
    "        for p in paragrahps:\n",
    "            for s in sent_tokenize(p):\n",
    "                sentences.append(s)\n",
    "        return sentences\n",
    "\n",
    "    @staticmethod\n",
    "    @logging\n",
    "    def remove_hyperlinks(text):\n",
    "        regex = r\"((https?:\\/\\/)?[^\\s]+\\.[^\\s]+)\"\n",
    "        text = re.sub(regex, '', text)\n",
    "        return text\n",
    "\n",
    "    @staticmethod\n",
    "    def to_lower(w, lower):\n",
    "        return w.lower() if lower else w\n",
    "    \n",
    "    @staticmethod\n",
    "    @logging\n",
    "    def vocab(file_path, file_type=\"csv\", lower=True):\n",
    "        nlp = StanfordCoreNLP(Utils.CORE_NLP_DIR)\n",
    "        wordsCount = Counter()\n",
    "        permissions = []\n",
    "        distincts_permissions = set()\n",
    "        if file_type == \"csv\":\n",
    "            with open(file_path) as f:\n",
    "                reader = csv.reader(f)\n",
    "                next(reader) # skip header\n",
    "                for row in reader:\n",
    "                    text = row[1]\n",
    "                    for sentence in Utils.preprocess(text):\n",
    "                        sentence = Utils.remove_hyperlinks(sentence)\n",
    "                        for w in nlp.word_tokenize(sentence):\n",
    "                            wordsCount.update([Utils.to_lower(w, lower)])\n",
    "                        for p in  row[2].strip().split(\",\"):\n",
    "                            ptype = Utils.to_lower(p, lower)\n",
    "                            if ptype not in distincts_permissions:\n",
    "                                pphrase = [Utils.to_lower(t, lower) for t in p.split(\"_\")]\n",
    "                                perm = Permission(ptype, pphrase)\n",
    "                                permissions.append(perm)\n",
    "                                distincts_permissions.add(ptype)\n",
    "                            for token in p.split(\"_\"):\n",
    "                                wordsCount.update([Utils.to_lower(token, lower)])\n",
    "        elif file_type == \"excel\":\n",
    "            handtagged_permissions = [\"READ_CALENDAR\", \"READ_CONTACTS\", \"RECORD_AUDIO\"]\n",
    "            loc = (file_path)\n",
    "            wb = xlrd.open_workbook(loc) \n",
    "            sheet = wb.sheet_by_index(0)\n",
    "            sharp_count = 0\n",
    "            apk_title = \"\"\n",
    "            for i in range(sheet.nrows):\n",
    "                sentence = sheet.cell_value(i, 0)\n",
    "                if sentence.startswith(\"##\"):\n",
    "                    sharp_count += 1\n",
    "                    if sharp_count % 2 == 1:\n",
    "                        apk_title = sentence.split(\"##\")[1] \n",
    "                else:\n",
    "                    if sharp_count != 0 and sharp_count % 2 == 0:\n",
    "                        sentence = sentence.strip()\n",
    "                        for w in nlp.word_tokenize(sentence):\n",
    "                            wordsCount.update([Utils.to_lower(w, lower)])\n",
    "                        for p in handtagged_permissions:\n",
    "                            ptype = Utils.to_lower(p, lower)\n",
    "                            if ptype not in distincts_permissions:\n",
    "                                pphrase = [Utils.to_lower(t, lower) for t in p.split(\"_\")]\n",
    "                                perm = Permission(ptype, pphrase)\n",
    "                                permissions.append(perm)\n",
    "                                distincts_permissions.add(ptype)\n",
    "                                for token in p.split(\"_\"):\n",
    "                                    wordsCount.update([Utils.to_lower(token, lower)])            \n",
    "        else:\n",
    "            raise Exception(\"Unsupported file type.\")\n",
    "        nlp.close()\n",
    "        return wordsCount.keys(), {w: i for i, w in enumerate(list(wordsCount.keys()))}, permissions\n",
    "\n",
    "    @staticmethod\n",
    "    @logging\n",
    "    def read_file(file_path, w2i, file_type=\"csv\", lower=True):\n",
    "        data = []\n",
    "        doc_id = 0\n",
    "        if file_type == \"csv\":\n",
    "            with open(file_path) as f:\n",
    "                reader = csv.reader(f)\n",
    "                next(reader) # skip header\n",
    "                for row in reader:\n",
    "                    doc_id += 1\n",
    "                    title = row[0]\n",
    "                    description = row[1]\n",
    "                    permissions = []\n",
    "                    for p in  row[2].strip().split(\",\"):\n",
    "                        ptype = Utils.to_lower(p, lower)\n",
    "                        pphrase = [Utils.to_lower(t, lower) for t in p.split(\"_\")]\n",
    "                        perm = Permission(ptype, pphrase)\n",
    "                        permissions.append(perm)\n",
    "\n",
    "                    sentences = []\n",
    "                    for sentence in Utils.preprocess(description):\n",
    "                        sentence = Utils.remove_hyperlinks(sentence)\n",
    "                        sentences.append(sentence.strip())\n",
    "                        ###sentences.append([Utils.to_lower(w, lower) for w in word_tokenize(sentence)])           \n",
    "                    yield Document(doc_id, title, sentences, permissions)\n",
    "                    \n",
    "        elif file_type == \"excel\":\n",
    "            permission_title = file_path.split(\"/\")[-1].split(\".\")[0]\n",
    "            loc = (file_path)\n",
    "            wb = xlrd.open_workbook(loc) \n",
    "            sheet = wb.sheet_by_index(0)\n",
    "            sharp_count = 0\n",
    "            title = \"\"\n",
    "            permissions = []\n",
    "            sentences = []\n",
    "            tags = []\n",
    "            for i in range(sheet.nrows):\n",
    "                sentence = sheet.cell_value(i, 0)\n",
    "                if sentence.startswith(\"##\"):\n",
    "                    sharp_count += 1\n",
    "                    if sharp_count % 2 == 1:\n",
    "                        if doc_id > 0:\n",
    "                            yield Document(doc_id, title, sentences, permissions, tags)\n",
    "                        \n",
    "                        #Document init values\n",
    "                        title = sentence.split(\"##\")[1]\n",
    "                        permissions = []\n",
    "                        sentences = []\n",
    "                        tags = []\n",
    "                        doc_id += 1\n",
    "                        \n",
    "                        # Permissions for apk\n",
    "                        ptype = Utils.to_lower(permission_title, lower)\n",
    "                        pphrase = [Utils.to_lower(t, lower) for t in permission_title.split(\"_\")]\n",
    "                        perm = Permission(ptype, pphrase)\n",
    "                        permissions.append(perm)\n",
    "                else:\n",
    "                    if sharp_count != 0 and sharp_count % 2 == 0:\n",
    "                        sentences.append(sentence.strip())\n",
    "                        ###sentences.append([Utils.to_lower(w, lower) for w in word_tokenize(sentence.strip())]) \n",
    "                        tags.append(int(sheet.cell_value(i, 1)))\n",
    "                        \n",
    "            yield Document(doc_id, title, sentences, permissions, tags)\n",
    "            wb.release_resources()\n",
    "            del wb\n",
    "        else:\n",
    "            raise Exception(\"Unsupported file type.\")\n",
    "    \n",
    "    @staticmethod\n",
    "    @logging\n",
    "    def get_data(file_path, w2i, sequence_type=\"dependency\", file_type=\"csv\", window_size=2, lower=True):\n",
    "        nlp = StanfordCoreNLP(Utils.CORE_NLP_DIR)\n",
    "        if sequence_type == \"raw\":\n",
    "            return Utils.read_file_raw(file_path, w2i, nlp, file_type, lower)\n",
    "        elif sequence_type == \"dependency\":\n",
    "            return Utils.read_file_dependency(file_path, w2i, nlp, file_type, lower)\n",
    "        elif sequence_type == \"windowed\":\n",
    "            return Utils.read_file_window(file_path, w2i, nlp, file_type, window_size, lower)\n",
    "        else:\n",
    "            nlp.close()\n",
    "            raise Exception(\"Unknown sequence type\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def read_file_raw(file_path, w2i, nlp, file_type=\"csv\", lower=True):\n",
    "        for doc in Utils.read_file(file_path, w2i, file_type, lower):\n",
    "            doc.description = [[Utils.to_lower(w, lower) for w in nlp.word_tokenize(sentence)] for sentence in doc.description]\n",
    "            yield doc\n",
    "        \n",
    "    @staticmethod\n",
    "    def read_file_window(file_path, w2i, nlp, file_type=\"csv\", window_size=2, lower=True):\n",
    "        for doc in Utils.read_file(file_path, w2i, file_type, lower):\n",
    "            doc.description = [[Utils.to_lower(w, lower) for w in nlp.word_tokenize(sentence)] for sentence in doc.description]\n",
    "            doc.description = Utils.split_into_windows(doc.description, window_size)\n",
    "            yield doc\n",
    "            \n",
    "    @staticmethod\n",
    "    def read_file_dependency(file_path, w2i, nlp, file_type=\"csv\", lower=True):\n",
    "        for doc in Utils.read_file(file_path, w2i, file_type, lower):\n",
    "            doc.description = Utils.split_into_dependencies(doc.description, nlp)\n",
    "            yield doc\n",
    "            \n",
    "    @staticmethod\n",
    "    def split_into_dependencies(sentences, nlp):\n",
    "        splitted_sentences = []\n",
    "        for sentence in sentences:\n",
    "            tokens = nlp.word_tokenize(sentence)\n",
    "            s = [[tokens[rel[1]-1], tokens[rel[2]-1]] for rel in nlp.dependency_parse(sentence) if rel[0] != 'ROOT']\n",
    "            splitted_sentences.append(s)\n",
    "        return splitted_sentences\n",
    "    \n",
    "    @staticmethod\n",
    "    def split_into_windows(sentences, window_size=2):\n",
    "        splitted_sentences = []\n",
    "        for sentence in sentences:\n",
    "            splitted_sentences.append([])\n",
    "            if len(sentence) < window_size:\n",
    "                splitted_sentences[-1].append(sentence)\n",
    "            else:\n",
    "                for start in range(len(sentence) - window_size + 1):\n",
    "                    splitted_sentences[-1].append([sentence[i+start] for i in range(window_size)])\n",
    "        return splitted_sentences\n",
    "    \n",
    "    @staticmethod     \n",
    "    def load_embeddings_file(file_name, embedding_type, lower=True):\n",
    "        if not os.path.isfile(file_name):\n",
    "            print(file_name, \"does not exist\")\n",
    "            return {}, 0\n",
    "\n",
    "        if embedding_type == \"word2vec\":\n",
    "            model = KeyedVectors.load_word2vec_format(file_name, binary=True, unicode_errors=\"ignore\")\n",
    "            words = model.index2entity\n",
    "        elif embedding_type == \"fasttext\":\n",
    "            model = FastText.load_fasttext_format(file_name)\n",
    "            words = [w for w in model.wv.vocab]\n",
    "        elif embedding_type == \"pickle\":\n",
    "            with open(file_name,'rb') as fp:\n",
    "                model = pickle.load(fp)\n",
    "                words = model.keys()\n",
    "        else:\n",
    "            raise Exception(\"Unknown Type\")\n",
    "            return {}, 0\n",
    "\n",
    "        if lower:\n",
    "            vectors = {word.lower(): model[word] for word in words}\n",
    "        else:\n",
    "            vectors = {word: model[word] for word in words}\n",
    "\n",
    "        if \"UNK\" not in vectors:\n",
    "            unk = np.mean([vectors[word] for word in vectors.keys()], axis=0)\n",
    "            vectors[\"UNK\"] = unk\n",
    "\n",
    "        return vectors, len(vectors[\"UNK\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptionParser:\n",
    "    def __init__(self, train, train_file_type, external_embedding, external_embedding_type, wembedding_dims, lstm_dims):\n",
    "        self.train = train\n",
    "        self.train_file_type = train_file_type\n",
    "        self.external_embedding = external_embedding\n",
    "        self.external_embedding_type = external_embedding_type\n",
    "        self.wembedding_dims = wembedding_dims\n",
    "        self.lstm_dims = lstm_dims\n",
    "    \n",
    "options = OptionParser(\"/home/huseyin/Desktop/Security/data/whyper/Read_Calendar.xls\",\n",
    "                       \"excel\",\n",
    "                       \"/home/huseyin/Desktop/Security/data/subset_fasttext.bin\",\n",
    "                       \"pickle\",\n",
    "                       300,\n",
    "                       128)\n",
    "\n",
    "\n",
    "ext_embeddings_arg, ext_emb_dim_arg = Utils.load_embeddings_file(options.external_embedding, options.external_embedding_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dynet_config\n",
    "# Declare GPU as the default device type\n",
    "dynet_config.set_gpu()\n",
    "# Set some parameters manualy\n",
    "dynet_config.set(mem=400,random_seed=9)\n",
    "# Initialize dynet import using above configuration in the current scope\n",
    "\n",
    "import dynet as dy\n",
    "\n",
    "from numpy import inf\n",
    "import random\n",
    "random.seed(33)\n",
    "\n",
    "class SimpleModel:\n",
    "    def __init__(self, vocab, w2i, permissions, options, ext_embeddings_arg, ext_emb_dim_arg):\n",
    "        self.model = dy.ParameterCollection()\n",
    "        self.trainer = dy.AdamTrainer(self.model)\n",
    "        \n",
    "        self.w2i = w2i\n",
    "        self.wdims = options.wembedding_dims\n",
    "        self.ldims = options.lstm_dims\n",
    "        self.all_permissions = permissions\n",
    "        self.train_file_type = options.train_file_type\n",
    "        \n",
    "        self.wlookup = self.model.add_lookup_parameters((len(w2i), self.wdims)) #PAD, and INITIAL tokens?\n",
    "        if options.external_embedding is not None:\n",
    "            ext_embeddings, ext_emb_dim =  ext_embeddings_arg, ext_emb_dim_arg #Utils.load_embeddings_file(options.external_embedding, options.external_embedding_type)\n",
    "            assert (ext_emb_dim == self.wdims)\n",
    "            print(\"Initializing word embeddings by pre-trained vectors\")\n",
    "            count = 0\n",
    "            for word in self.w2i:\n",
    "                if word in ext_embeddings:\n",
    "                    count += 1\n",
    "                    self.wlookup.init_row(self.w2i[word], ext_embeddings[word])\n",
    "            self.ext_embeddings = ext_embeddings\n",
    "            print(\"Vocab size: %d; #words having pretrained vectors: %d\" % (len(self.w2i), count))\n",
    "            \n",
    "        self.sentence_rnn = [dy.SimpleRNNBuilder(1, self.wdims, self.ldims, self.model)] # Try bi-rnn and lstm\n",
    "        self.permission_rrn = [dy.SimpleRNNBuilder(1, self.wdims, self.ldims, self.model)] # Try bi-rnn and lstm\n",
    "        \n",
    "        self.mlp_w = self.model.add_parameters((128, self.ldims))\n",
    "        self.mlp_b = self.model.add_parameters(128)\n",
    "        self.mlp_v = self.model.add_parameters((1, 128))\n",
    "    \n",
    "    def cos_similiariy(self, v1, v2):\n",
    "        from numpy import dot\n",
    "        from numpy.linalg import norm\n",
    "        return dot(v1, v2)/(norm(v1)*norm(v2))\n",
    "    \n",
    "    def cosine_proximity(self, pred, gold):\n",
    "        def l2_normalize(x):\n",
    "            square_sum = dy.sqrt(dy.bmax(dy.sum_elems(dy.square(x)), np.finfo(float).eps * dy.ones((1))[0]))\n",
    "            return dy.cdiv(x, square_sum)\n",
    "\n",
    "        y_true = l2_normalize(pred)\n",
    "        y_pred = l2_normalize(gold)\n",
    "\n",
    "        return -dy.sum_elems(dy.cmult(y_true, y_pred))\n",
    "    \n",
    "    def cosine_loss(self, pred, gold):\n",
    "        return dy.cdiv(dy.dot_product(pred,gold), dy.cmult(dy.squared_norm(pred), dy.squared_norm(gold)))\n",
    "    \n",
    "    def description_permission_sim_w_max(self, sentences, perm):\n",
    "        max_sim = -inf\n",
    "        for sentence_enc in sentences:\n",
    "            sim = self.cos_similiariy(sentence_enc, perm)\n",
    "            if max_sim < sim: max_sim = sim\n",
    "        return max_sim\n",
    "\n",
    "    def statistics(self, similarities):\n",
    "        statistics = {}\n",
    "        for app_id in similarities.keys():\n",
    "            statistics[app_id] = {\"related\": {\"max\" : None, \"avg\" : None, \"all\" : []},\n",
    "                                  \"unrelated\": {\"max\" : None, \"avg\" : None, \"all\" : []}}\n",
    "\n",
    "            max_related, max_unrelated = -inf, -inf\n",
    "            avg_related, avg_unrelated = 0, 0\n",
    "            for related_p in similarities[app_id][\"related\"]:\n",
    "                if max_related < related_p[1]: \n",
    "                    max_related = related_p[1]\n",
    "                avg_related += related_p[1]\n",
    "                statistics[app_id][\"related\"][\"all\"].append(related_p[1])\n",
    "\n",
    "            for unrelated_p in similarities[app_id][\"unrelated\"]:\n",
    "                if max_unrelated < unrelated_p[1]: \n",
    "                    max_unrelated = unrelated_p[1]\n",
    "                avg_unrelated += unrelated_p[1]\n",
    "                statistics[app_id][\"unrelated\"][\"all\"].append(unrelated_p[1])\n",
    "\n",
    "            statistics[app_id][\"related\"][\"max\"] = max_related\n",
    "            statistics[app_id][\"unrelated\"][\"max\"] = max_unrelated\n",
    "            statistics[app_id][\"related\"][\"avg\"] = avg_related / len(similarities[app_id][\"related\"])\n",
    "            statistics[app_id][\"unrelated\"][\"avg\"] = avg_unrelated / len(similarities[app_id][\"unrelated\"])\n",
    "        return statistics\n",
    "\n",
    "    def statistics_gold(self, similarities):\n",
    "        statistics = {}\n",
    "        for app_id in similarities.keys():\n",
    "            statistics[app_id] = {\"related\": { \"all\" : []},\n",
    "                                  \"unrelated\": {\"all\" : []}}\n",
    "            max_related, max_unrelated = -inf, -inf\n",
    "            avg_related, avg_unrelated = 0, 0\n",
    "            for related_p in similarities[app_id][\"related\"]:\n",
    "                statistics[app_id][\"related\"][\"all\"].append(related_p[1])\n",
    "\n",
    "            for unrelated_p in similarities[app_id][\"unrelated\"]:\n",
    "                statistics[app_id][\"unrelated\"][\"all\"].append(unrelated_p[1])\n",
    "        return statistics\n",
    "\n",
    "    def train(self, file_path):\n",
    "        document_permission_similiarities = {}\n",
    "        permission_vecs = {}\n",
    "        # gather all permission encoding of permissions\n",
    "        for perm  in self.all_permissions:\n",
    "            rnn_forward = self.permission_rrn[0].initial_state()\n",
    "            for entry in perm.pphrase:\n",
    "                vec = self.wlookup[int(self.w2i.get(entry, 0))]\n",
    "                rnn_forward = rnn_forward.add_input(vec)\n",
    "            permission_vecs[perm.ptype] = rnn_forward.output().npvalue()\n",
    "            dy.renew_cg()\n",
    "\n",
    "        for doc in Utils.read_file(file_path, self.w2i, file_type=self.train_file_type):\n",
    "            if doc.description:                \n",
    "                #Sentence encoding\n",
    "                sentence_enc_s = []\n",
    "                for sentence in doc.descriptions:\n",
    "                    rnn_forward = self.sentence_rnn[0].initial_state()\n",
    "                    for entry in sentence:\n",
    "                        vec = self.wlookup[int(self.w2i.get(entry, 0))]\n",
    "                        rnn_forward = rnn_forward.add_input(vec)\n",
    "                    if rnn_forward.output() is not None:\n",
    "                        sentence_enc_s.append(rnn_forward.output().npvalue())\n",
    "                    dy.renew_cg()\n",
    "                    \n",
    "                document_permission_similiarities[doc.id] = {\"related\": [], \"unrelated\" : []}\n",
    "                app_permissions = set()\n",
    "                for related_p in doc.permissions:\n",
    "                    sim = self.description_permission_sim_w_max(sentence_enc_s, permission_vecs[related_p.ptype])\n",
    "                    document_permission_similiarities[doc.id][\"related\"].append((related_p.ptype, sim))\n",
    "                    app_permissions.add(related_p.ptype)\n",
    "                for unrelated_p in self.all_permissions:\n",
    "                    if unrelated_p.ptype not in app_permissions:\n",
    "                        sim = self.description_permission_sim_w_max(sentence_enc_s, permission_vecs[unrelated_p.ptype])\n",
    "                        document_permission_similiarities[doc.id][\"unrelated\"].append((unrelated_p.ptype, sim))\n",
    "                \n",
    "        return document_permission_similiarities\n",
    "\n",
    "    def train_gold(self, file_path):\n",
    "        document_permission_similiarities = {}\n",
    "        permission_vecs = {}\n",
    "        # gather all permission encoding of permissions\n",
    "        for perm  in self.all_permissions:\n",
    "            rnn_forward = self.permission_rrn[0].initial_state()\n",
    "            for entry in perm.pphrase:\n",
    "                vec = self.wlookup[int(self.w2i.get(entry, 0))]\n",
    "                rnn_forward = rnn_forward.add_input(vec)\n",
    "            permission_vecs[perm.ptype] = rnn_forward.output().npvalue()\n",
    "            dy.renew_cg()\n",
    "\n",
    "        for doc in Utils.read_file(file_path, self.w2i, file_type=self.train_file_type):\n",
    "            if doc.description:                \n",
    "                #Sentence encoding\n",
    "                sentence_enc_s = []\n",
    "                for sentence,tag in zip(doc.description, doc.tags):\n",
    "                    if tag != 0 and tag != 4:\n",
    "                        rnn_forward = self.sentence_rnn[0].initial_state()\n",
    "                        for entry in sentence:\n",
    "                            vec = self.wlookup[int(self.w2i.get(entry, 0))]\n",
    "                            rnn_forward = rnn_forward.add_input(vec)\n",
    "                        if rnn_forward.output() is not None:\n",
    "                            sentence_enc_s.append(rnn_forward.output().npvalue())\n",
    "                        dy.renew_cg()\n",
    "                    \n",
    "                document_permission_similiarities[doc.id] = {\"related\": [], \"unrelated\" : []}\n",
    "\n",
    "\n",
    "                app_permissions = set()\n",
    "                for related_p in doc.permissions:\n",
    "                    document_permission_similiarities[doc.id][\"related\"].extend([(related_p.ptype, self.cos_similiariy(sentence_enc, permission_vecs[related_p.ptype])) for sentence_enc in sentence_enc_s])\n",
    "                    app_permissions.add(related_p.ptype)\n",
    "                for unrelated_p in self.all_permissions:\n",
    "                    if unrelated_p.ptype not in app_permissions:\n",
    "                        document_permission_similiarities[doc.id][\"unrelated\"].extend([(unrelated_p.ptype, self.cos_similiariy(sentence_enc, permission_vecs[unrelated_p.ptype])) for sentence_enc in sentence_enc_s])\n",
    "                \n",
    "        return document_permission_similiarities\n",
    "    \n",
    "    @logging\n",
    "    def train_gold_splitted(self, documents): \n",
    "        for doc in documents:\n",
    "            if doc.description:                \n",
    "                #Sentence encoding\n",
    "                sentence_enc_s = []\n",
    "                for sentence,tag in zip(doc.description, doc.tags):\n",
    "                    sentence_enc_s.append([])\n",
    "                    if tag == 1 or tag == 2 or tag == 3:\n",
    "                        for window in sentence:\n",
    "                            permission_vecs = {}\n",
    "                            # gather all permission encoding of permissions\n",
    "                            for perm  in self.all_permissions:\n",
    "                                rnn_forward = self.permission_rrn[0].initial_state()\n",
    "                                for entry in perm.pphrase:\n",
    "                                    vec = self.wlookup[int(self.w2i.get(entry, 0))]\n",
    "                                    rnn_forward = rnn_forward.add_input(vec)\n",
    "                                permission_vecs[perm.ptype] = rnn_forward.output()\n",
    "                                \n",
    "                            rnn_forward = self.sentence_rnn[0].initial_state()\n",
    "                            for entry in window:\n",
    "                                vec = self.wlookup[int(self.w2i.get(entry, 0))]\n",
    "                                rnn_forward = rnn_forward.add_input(vec)\n",
    "                            \n",
    "                            loss = 1-self.cosine_loss(rnn_forward.output(), permission_vecs[doc.permissions[0].ptype])\n",
    "                            loss.backward()\n",
    "                            self.trainer.update()\n",
    "                            dy.renew_cg()\n",
    "                    \n",
    "                    elif tag == 0:\n",
    "                        for window in sentence:\n",
    "                            permission_vecs = {}\n",
    "                            # gather all permission encoding of permissions\n",
    "                            for perm  in self.all_permissions:\n",
    "                                rnn_forward = self.permission_rrn[0].initial_state()\n",
    "                                for entry in perm.pphrase:\n",
    "                                    vec = self.wlookup[int(self.w2i.get(entry, 0))]\n",
    "                                    rnn_forward = rnn_forward.add_input(vec)\n",
    "                                permission_vecs[perm.ptype] = rnn_forward.output()\n",
    "                                \n",
    "                            rnn_forward = self.sentence_rnn[0].initial_state()\n",
    "                            for entry in window:\n",
    "                                vec = self.wlookup[int(self.w2i.get(entry, 0))]\n",
    "                                rnn_forward = rnn_forward.add_input(vec)\n",
    "                           \n",
    "                            loss = self.cosine_loss(rnn_forward.output(), permission_vecs[Utils.to_lower(\"READ_CONTACTS\", True)])\n",
    "                            loss += self.cosine_loss(rnn_forward.output(), permission_vecs[Utils.to_lower(\"RECORD_AUDIO\", True)])\n",
    "                            \n",
    "                            loss.backward()\n",
    "                            self.trainer.update()\n",
    "                            dy.renew_cg()\n",
    "                            \n",
    "    @logging\n",
    "    def test_gold_splitted(self, documents):\n",
    "        document_permission_similiarities = {}\n",
    "        permission_vecs = {}\n",
    "        # gather all permission encoding of permissions\n",
    "        for perm  in self.all_permissions:\n",
    "            rnn_forward = self.permission_rrn[0].initial_state()\n",
    "            for entry in perm.pphrase:\n",
    "                vec = self.wlookup[int(self.w2i.get(entry, 0))]\n",
    "                rnn_forward = rnn_forward.add_input(vec)\n",
    "            permission_vecs[perm.ptype] = rnn_forward.output().npvalue()\n",
    "        for doc in documents:\n",
    "            if doc.description:                \n",
    "                #Sentence encoding\n",
    "                sentence_enc_s = []\n",
    "                for sentence,tag in zip(doc.description, doc.tags):\n",
    "                    sentence_enc_s.append([])\n",
    "                    if tag == 1 or tag == 2 or tag == 3:\n",
    "                        for window in sentence:\n",
    "                            rnn_forward = self.sentence_rnn[0].initial_state()\n",
    "                            for entry in window:\n",
    "                                vec = self.wlookup[int(self.w2i.get(entry, 0))]\n",
    "                                rnn_forward = rnn_forward.add_input(vec)\n",
    "                            if rnn_forward.output() is not None:\n",
    "                                rnn_forward.output().npvalue()\n",
    "                                sentence_enc_s[-1].append(rnn_forward.output().npvalue())\n",
    "                            dy.renew_cg()\n",
    "\n",
    "                document_permission_similiarities[doc.id] = {\"related\": [], \"unrelated\" : []}\n",
    "                app_permissions = set()\n",
    "                for related_p in doc.permissions:\n",
    "                    document_permission_similiarities[doc.id][\"related\"].extend([(related_p.ptype, self.description_permission_sim_w_max(sentence_enc, permission_vecs[related_p.ptype])) for sentence_enc in sentence_enc_s])\n",
    "                    app_permissions.add(related_p.ptype)\n",
    "                for unrelated_p in self.all_permissions:\n",
    "                    if unrelated_p.ptype not in app_permissions:\n",
    "                        document_permission_similiarities[doc.id][\"unrelated\"].extend([(unrelated_p.ptype, self.description_permission_sim_w_max(sentence_enc, permission_vecs[unrelated_p.ptype])) for sentence_enc in sentence_enc_s])\n",
    "        return document_permission_similiarities\n",
    "    \n",
    "    @logging\n",
    "    def train_test_splitted(self, file_path, window_size=2):\n",
    "        documents = []\n",
    "        for doc in Utils.get_data(file_path, self.w2i, sequence_type=\"windowed\", file_type=self.train_file_type, window_size=window_size, lower=True):\n",
    "            documents.append(doc)\n",
    "        random.shuffle(documents)\n",
    "        split_point = (3*len(documents))//4\n",
    "        return documents[:split_point], documents[split_point:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.42 s, sys: 511 ms, total: 4.93 s\n",
      "Wall time: 8.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "words, w2i, permissions = Utils.vocab(options.train, file_type=options.train_file_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing word embeddings by pre-trained vectors\n",
      "Vocab size: 4834; #words having pretrained vectors: 4289\n",
      "CPU times: user 86.6 ms, sys: 3.75 ms, total: 90.3 ms\n",
      "Wall time: 90.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = SimpleModel(words, w2i, permissions, options, ext_embeddings_arg, ext_emb_dim_arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Related all 47 6.414294843333786\n",
      "Unrelated all 94 14.119336208219256\n",
      "Epoch 1\n",
      "Related all 47 14.376620473055205\n",
      "Unrelated all 94 -19.333111361101007\n",
      "Epoch 2\n",
      "Related all 47 12.055685355669828\n",
      "Unrelated all 94 -32.3858667674506\n",
      "Epoch 3\n",
      "Related all 47 8.972893063728412\n",
      "Unrelated all 94 -26.62829052708988\n",
      "Epoch 4\n",
      "Related all 47 8.841505251565907\n",
      "Unrelated all 94 -26.52606014423365\n",
      "Epoch 5\n",
      "Related all 47 27.643595737420558\n",
      "Unrelated all 94 -25.94652449732265\n",
      "Epoch 6\n",
      "Related all 47 11.090441190876653\n",
      "Unrelated all 94 -32.67492675701615\n",
      "Epoch 7\n",
      "Related all 47 21.550440314474304\n",
      "Unrelated all 94 -27.676381941857844\n",
      "Epoch 8\n",
      "Related all 47 13.752036448597378\n",
      "Unrelated all 94 -13.542136263258612\n",
      "Epoch 9\n",
      "Related all 47 11.154242065755676\n",
      "Unrelated all 94 -32.77386143732741\n",
      "Epoch 10\n",
      "Related all 47 12.697116208575821\n",
      "Unrelated all 94 -16.241865823022103\n",
      "CPU times: user 3min 14s, sys: 652 ms, total: 3min 14s\n",
      "Wall time: 3min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "@logging\n",
    "def draw_histogram(data, img_name):\n",
    "    stats = model.statistics_gold(data)   \n",
    "    related_all = []\n",
    "    unrelated_all = []\n",
    "    for doc_id in stats:\n",
    "        related_all.extend([i for i in stats[doc_id][\"related\"][\"all\"] if i > -inf])\n",
    "        unrelated_all.extend([i for i in stats[doc_id][\"unrelated\"][\"all\"] if i > -inf])\n",
    "    print(\"Related all\", len(related_all), sum(related_all))\n",
    "    print(\"Unrelated all\", len(unrelated_all), sum(unrelated_all))\n",
    "\n",
    "    from matplotlib import pyplot\n",
    "\n",
    "    pyplot.title(\"All similarity\")\n",
    "    pyplot.hist(related_all, bins='auto', alpha=0.5, label='related')\n",
    "    pyplot.hist(unrelated_all, bins='auto', alpha=0.5, label='unrelated')\n",
    "    pyplot.legend(loc='upper right')\n",
    "    pyplot.savefig(img_name)\n",
    "    pyplot.clf()\n",
    "\n",
    "train_data, test_data = model.train_test_splitted(options.train)\n",
    "similarities = model.test_gold_splitted(test_data)\n",
    "draw_histogram(similarities, \"trained_epoch_{}.png\".format(0))\n",
    "for i in range(10):\n",
    "    print(\"Epoch {}\".format(i+1))\n",
    "    model.train_gold_splitted(train_data)\n",
    "    similarities = model.test_gold_splitted(test_data)\n",
    "    draw_histogram(similarities, \"trained_epoch_{}.png\".format(i+1))\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
