{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDownloading emoji data ...\u001b[0m\n",
      "\u001b[92m... OK\u001b[0m (Got response in 0.39 seconds)\n",
      "\u001b[33mWriting emoji data to /home/huseyinalecakir/.demoji/codes.json ...\u001b[0m\n",
      "\u001b[92m... OK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "get_ipython().run_line_magic('autoreload', '2')\n",
    "import sys\n",
    "from numpy import inf\n",
    "from scripts.similarity_experiment import SimilarityExperiment\n",
    "\n",
    "from model.rnn_model import RNNModel\n",
    "from utils.io_utils import IOUtils\n",
    "\n",
    "\n",
    "class ArgumentParser():\n",
    "    permission_type = \"READ_CONTACTS\"\n",
    "    train = \"/home/huseyinalecakir/Security/data/ac-net/ACNET_DATASET.csv\"\n",
    "    train_file_type = \"acnet\"\n",
    "    test = \"/home/huseyinalecakir/Security/data/whyper/Read_Contacts.csv\"\n",
    "    test_file_type = \"whyper\"\n",
    "    external_embedding = \"/home/huseyinalecakir/Security/data/{}\".format(\"cc.en.300.bin\")\n",
    "    external_embedding_type = \"fasttext\"\n",
    "    wembedding_dims = 300\n",
    "    lstm_dims = 128\n",
    "    sequence_type = \"windowed\"\n",
    "    window_size = 2\n",
    "    saved_parameters_dir = \"/home/huseyinalecakir/Security/data/saved_parameters/{}/{}\".format(permission_type, external_embedding_type)\n",
    "    saved_prevectors = \"embeddings.pickle\"\n",
    "    saved_vocab_test = \"whyper-vocab.txt\"\n",
    "    saved_vocab_train = \"acnet-vocab.txt\"\n",
    "    saved_preprocessed_whyper = \"whyper-preprocessed.txt\"\n",
    "    saved_preprocessed_acnet = \"acnet-preprocessed.txt\"\n",
    "    outdir = \"./test/{}/{}\".format(permission_type, external_embedding_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import dynet_config\n",
    "# Declare GPU as the default device type\n",
    "dynet_config.set_gpu()\n",
    "# Set some parameters manualy\n",
    "dynet_config.set(mem=400, random_seed=123456789)\n",
    "# Initialize dynet import using above configuration in the current scope\n",
    "\n",
    "import scipy\n",
    "import dynet as dy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils.io_utils import IOUtils\n",
    "from utils.nlp_utils import NLPUtils\n",
    "\n",
    "\n",
    "random.seed(33)\n",
    "\n",
    "\n",
    "class SentenceReport:\n",
    "    \"\"\"TODO\"\"\"\n",
    "    def __init__(self, sentence, mark):\n",
    "        self.mark = mark\n",
    "        self.preprocessed_sentence = None\n",
    "        self.sentence = sentence\n",
    "        self.all_phrases = None\n",
    "        self.feature_weights = None\n",
    "        self.max_similarites = None\n",
    "        self.prediction_result = None\n",
    "\n",
    "        \n",
    "class SimilarityExperiment:\n",
    "    \"\"\"TODO\"\"\"\n",
    "    def __init__(self, w2i, options):\n",
    "        print('Similarity Experiment - init')\n",
    "        self.options = options\n",
    "        self.model = dy.ParameterCollection()\n",
    "        self.trainer = dy.AdamTrainer(self.model)\n",
    "        self.w2i = w2i\n",
    "        self.wdims = options.wembedding_dims\n",
    "        self.ldims = options.lstm_dims\n",
    "\n",
    "        self.ext_embeddings = None\n",
    "        #Model Parameters\n",
    "        self.wlookup = self.model.add_lookup_parameters((len(w2i), self.wdims))\n",
    "\n",
    "        self.__load_model()\n",
    "\n",
    "        self.phrase_rnn = [dy.VanillaLSTMBuilder(1, self.wdims, self.ldims, self.model)]\n",
    "        self.mlp_w = self.model.add_parameters((1, self.ldims))\n",
    "        self.mlp_b = self.model.add_parameters(1)\n",
    "\n",
    "    def __load_model(self):\n",
    "        if self.options.external_embedding is not None:\n",
    "            if os.path.isfile(os.path.join(self.options.saved_parameters_dir,\n",
    "                                           self.options.saved_prevectors)):\n",
    "                self.__load_external_embeddings(os.path.join(self.options.saved_parameters_dir,\n",
    "                                                             self.options.saved_prevectors),\n",
    "                                                \"pickle\")\n",
    "            else:\n",
    "                self.__load_external_embeddings(self.options.external_embedding,\n",
    "                                                self.options.external_embedding_type)\n",
    "                self.__save_model()\n",
    "\n",
    "    def __save_model(self):\n",
    "        IOUtils.save_embeddings(os.path.join(self.options.saved_parameters_dir,\n",
    "                                             self.options.saved_prevectors),\n",
    "                                self.ext_embeddings)\n",
    "\n",
    "    def __load_external_embeddings(self, embedding_file, embedding_file_type):\n",
    "        ext_embeddings, ext_emb_dim = IOUtils.load_embeddings_file(\n",
    "            embedding_file,\n",
    "            embedding_file_type,\n",
    "            lower=True)\n",
    "        assert ext_emb_dim == self.wdims\n",
    "        self.ext_embeddings = {}\n",
    "        print(\"Initializing word embeddings by pre-trained vectors\")\n",
    "        count = 0\n",
    "        for word in self.w2i:\n",
    "            if word in ext_embeddings:\n",
    "                count += 1\n",
    "                self.ext_embeddings[word] = ext_embeddings[word]\n",
    "                self.wlookup.init_row(self.w2i[word], ext_embeddings[word])\n",
    "        print(\"Vocab size: %d; #words having pretrained vectors: %d\" % (len(self.w2i), count))\n",
    "\n",
    "\n",
    "def __split_into_windows(sentence, window_size):\n",
    "    splitted_sentences = []\n",
    "    if len(sentence) < window_size:\n",
    "        splitted_sentences.append(sentence)\n",
    "    else:\n",
    "        for start in range(len(sentence) - window_size + 1):\n",
    "            splitted_sentences.append([sentence[i+start] for i in range(window_size)])\n",
    "    return splitted_sentences\n",
    "\n",
    "def __find_all_possible_phrases(sentence, sentence_only=False):\n",
    "    entries = sentence.split(\" \")\n",
    "    all_phrases = []\n",
    "    if sentence_only:\n",
    "        all_phrases.extend(__split_into_windows(entries, len(entries)))\n",
    "    else:\n",
    "        for windows_size in range(2, len(entries)+1):\n",
    "            all_phrases.extend(__split_into_windows(entries, windows_size))\n",
    "    return all_phrases\n",
    "\n",
    "def __train(model, data):\n",
    "    def encode_sequence(seq):\n",
    "        rnn_forward = model.phrase_rnn[0].initial_state()\n",
    "        for entry in seq:\n",
    "            vec = model.wlookup[int(model.w2i.get(entry, 0))]\n",
    "            rnn_forward = rnn_forward.add_input(vec)\n",
    "        return rnn_forward.output()\n",
    "    tagged_loss = 0\n",
    "    untagged_loss = 0\n",
    "    for index, sentence_report in enumerate(data):\n",
    "        for phrase in sentence_report.all_phrases:\n",
    "            loss = None\n",
    "            encoded_phrase = encode_sequence(phrase)\n",
    "            y_pred = dy.logistic((model.mlp_w*encoded_phrase) + model.mlp_b)\n",
    "\n",
    "            if sentence_report.mark:\n",
    "                loss = dy.binary_log_loss(y_pred, dy.scalarInput(1))\n",
    "            else:\n",
    "                loss = dy.binary_log_loss(y_pred, dy.scalarInput(0))\n",
    "\n",
    "            if sentence_report.mark:\n",
    "                tagged_loss += loss.scalar_value()/(index+1)\n",
    "            else:\n",
    "                untagged_loss += loss.scalar_value()/(index+1)\n",
    "            loss.backward()\n",
    "            model.trainer.update()\n",
    "            dy.renew_cg()\n",
    "\n",
    "def __predict(model, data):\n",
    "    def encode_sequence(seq):\n",
    "        rnn_forward = model.phrase_rnn[0].initial_state()\n",
    "        for entry in seq:\n",
    "            vec = model.wlookup[int(model.w2i.get(entry, 0))]\n",
    "            rnn_forward = rnn_forward.add_input(vec)\n",
    "        return rnn_forward.output()\n",
    "\n",
    "    for _, sentence_report in enumerate(data):\n",
    "        for phrase in sentence_report.all_phrases:\n",
    "            encoded_phrase = encode_sequence(phrase)\n",
    "            y_pred = dy.logistic((model.mlp_w*encoded_phrase) + model.mlp_b)\n",
    "            sentence_report.prediction_result = y_pred.scalar_value()\n",
    "            dy.renew_cg()\n",
    "\n",
    "def __report_confusion_matrix(sentence_reports, threshold):\n",
    "    results = {\"TP\":0, \"TN\":0, \"FP\":0, \"FN\":0, \"Threshold\":threshold}\n",
    "    total = 0\n",
    "    for report in sentence_reports:\n",
    "        total += 1\n",
    "        if report.mark:\n",
    "            if report.prediction_result >= threshold:\n",
    "                results[\"TP\"] += 1\n",
    "            else:\n",
    "                results[\"FN\"] += 1\n",
    "        else:\n",
    "            if report.prediction_result >= threshold:\n",
    "                results[\"FP\"] += 1\n",
    "            else:\n",
    "                results[\"TN\"] += 1\n",
    "    try:\n",
    "        results[\"precision\"] = results[\"TP\"]/(results[\"TP\"]+results[\"FP\"])\n",
    "        results[\"recall\"] = results[\"TP\"]/(results[\"TP\"]+results[\"FN\"])\n",
    "        results[\"f1_score\"] = 2*((results[\"precision\"]*results[\"recall\"])/(results[\"precision\"]+results[\"recall\"]))\n",
    "        results[\"accuracy\"] = (results[\"TP\"]+results[\"TN\"])/(results[\"TP\"]+results[\"TN\"]+results[\"FP\"]+results[\"FN\"])\n",
    "    except ZeroDivisionError:\n",
    "        results[\"precision\"] = 0\n",
    "        results[\"recall\"] = 0\n",
    "        results[\"f1_score\"] = 0\n",
    "        results[\"accuracy\"] = 0\n",
    "    return results\n",
    "\n",
    "def __save_preprocessed_file(outfile, reports, permission):\n",
    "    print(\"Saving {} for permission {}\".format(outfile, permission))\n",
    "    with open(outfile, \"w\") as target:\n",
    "        for report in reports:\n",
    "            if report.mark:\n",
    "                target.write(\"{}||{}||{}\\n\".format(report.sentence, report.preprocessed_sentence, permission))\n",
    "            else:\n",
    "                target.write(\"{}||{}||{}\\n\".format(report.sentence, report.preprocessed_sentence, \"NONE\"))\n",
    "\n",
    "def __load_preprocessed_file(infile, permission):\n",
    "    print(\"Loading {} for permission {}\".format(infile, permission))\n",
    "    sentence_reports = []\n",
    "    with open(infile, \"r\") as target:\n",
    "        for line in target:\n",
    "            t = line.strip().split(\"||\")    \n",
    "            sentence = t[0]\n",
    "            mark = None\n",
    "            if t[2] == permission:\n",
    "                mark = True\n",
    "            else:\n",
    "                mark = False\n",
    "            sentence_report = SentenceReport(sentence, mark)\n",
    "            sentence_report.preprocessed_sentence = t[1]\n",
    "            sentence_report.all_phrases = __find_all_possible_phrases( sentence_report.preprocessed_sentence,\n",
    "                                                                       sentence_only=True)\n",
    "            sentence_reports.append(sentence_report)\n",
    "    return sentence_reports\n",
    "\n",
    "def __load_row_whyper_file(infile):\n",
    "    print(\"Loading row {}\".format(infile))\n",
    "    tagged_test_file = pd.read_csv(infile)\n",
    "    test_sentence_reports = []\n",
    "    \n",
    "    #read and preprocess whyper sentences\n",
    "    print(\"Reading Test Sentences\")\n",
    "    for idx, row in tagged_test_file.iterrows():\n",
    "        sentence = str(row[\"Sentences\"])\n",
    "        if not sentence.startswith(\"#\"):\n",
    "            mark = None\n",
    "            if \"Manually Marked\" in row:\n",
    "                if row[\"Manually Marked\"] == 1:\n",
    "                    mark = True\n",
    "                else:\n",
    "                    mark = False\n",
    "            else:\n",
    "                raise Exception(\"Manually Marked label does not exist\")\n",
    "            sentence_report = SentenceReport(sentence, mark)\n",
    "            sentence_report.preprocessed_sentence = \" \".join(NLPUtils.preprocess_sentence(sentence_report.sentence))\n",
    "            sentence_report.all_phrases = __find_all_possible_phrases(sentence_report.preprocessed_sentence,\n",
    "                                                                      sentence_only=True)\n",
    "            test_sentence_reports.append(sentence_report)\n",
    "    return test_sentence_reports\n",
    "\n",
    "def __load_row_acnet_file(infile, gold_permission):\n",
    "    print(\"Loading row {} \".format(infile))\n",
    "    #read training data\n",
    "    print(\"Reading Train Sentences\")\n",
    "    tagged_train_file = pd.read_csv(infile)\n",
    "    train_sententence_reports = []\n",
    "    acnet_map = {\"RECORD_AUDIO\" : \"MICROPHONE\", \"READ_CONTACTS\": \"CONTACTS\", \"READ_CALENDAR\": \"CALENDAR\"}\n",
    "    for idx, row in tagged_train_file.iterrows():\n",
    "        sentence = row[\"sentence\"]\n",
    "        mark = None\n",
    "        if row[acnet_map[gold_permission]] is 1:\n",
    "            mark = True\n",
    "        else:\n",
    "            mark = False\n",
    "        sentence_report = SentenceReport(sentence, mark)\n",
    "        sentence_report.preprocessed_sentence = \" \".join(NLPUtils.preprocess_sentence(sentence_report.sentence))\n",
    "        sentence_report.all_phrases = __find_all_possible_phrases( sentence_report.preprocessed_sentence,\n",
    "                                                                    sentence_only=True)\n",
    "        train_sententence_reports.append(sentence_report)\n",
    "    return train_sententence_reports\n",
    "\n",
    "def __load_whyper_sentences(options, permission):\n",
    "    if options.saved_preprocessed_whyper is not None:\n",
    "        if os.path.isfile(os.path.join(options.saved_parameters_dir,\n",
    "                                       options.saved_preprocessed_whyper)):\n",
    "            return __load_preprocessed_file(os.path.join(options.saved_parameters_dir,\n",
    "                                                         options.saved_preprocessed_whyper), permission)\n",
    "        else:\n",
    "            reports = __load_row_whyper_file(options.test)\n",
    "            __save_preprocessed_file(os.path.join(options.saved_parameters_dir,\n",
    "                                                  options.saved_preprocessed_whyper), reports, permission)\n",
    "            return reports\n",
    "    else:\n",
    "        raise Exception(\"Set saved_preprocessed_whyper option\")\n",
    "\n",
    "def __load_acnet_sentences(options, permission):\n",
    "    if options.saved_preprocessed_acnet is not None:\n",
    "        if os.path.isfile(os.path.join(options.saved_parameters_dir,\n",
    "                                       options.saved_preprocessed_acnet)):\n",
    "            return __load_preprocessed_file(os.path.join(options.saved_parameters_dir,\n",
    "                                                 options.saved_preprocessed_acnet), permission)\n",
    "        else:\n",
    "            reports = __load_row_acnet_file(options.train, permission)\n",
    "            __save_preprocessed_file(os.path.join(options.saved_parameters_dir,\n",
    "                                                  options.saved_preprocessed_acnet), reports, permission)\n",
    "            return reports\n",
    "    else:\n",
    "        raise Exception(\"Set saved_preprocessed_acnet option\")\n",
    "def __load_sentences(options, permission, data_type):\n",
    "    if data_type == \"ACNET\":\n",
    "        return __load_acnet_sentences(options, permission)\n",
    "    elif data_type == \"WHYPER\":\n",
    "        return __load_whyper_sentences(options, permission)\n",
    "    else:\n",
    "        raise Exception(\"Unkown data type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ArgumentParser()\n",
    "print('Extracting training vocabulary')\n",
    "train_w2i, _ = IOUtils.load_vocab(  args.train,\n",
    "                                    args.train_file_type,\n",
    "                                    args.saved_parameters_dir,\n",
    "                                    args.saved_vocab_train,\n",
    "                                    args.external_embedding,\n",
    "                                    args.external_embedding_type,\n",
    "                                    True)\n",
    "\n",
    "print('Extracting test vocabulary')\n",
    "test_w2i, _ = IOUtils.load_vocab(args.test,\n",
    "                                 args.test_file_type,\n",
    "                                 args.saved_parameters_dir,\n",
    "                                 args.saved_vocab_test,\n",
    "                                 args.external_embedding,\n",
    "                                 args.external_embedding_type,\n",
    "                                 True)\n",
    "\n",
    "#combine test&train vocabulary\n",
    "w2i = train_w2i\n",
    "for token in test_w2i:\n",
    "    if token not in w2i:\n",
    "        w2i[token] = len(w2i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_sentences = __load_sentences(args,  args.permission_type, \"ACNET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve, precision_recall_curve\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "roc_scores = []\n",
    "pr_scores = []\n",
    "whole_sentences = np.array(whole_sentences)\n",
    "\n",
    "random.shuffle(whole_sentences)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimilarityExperiment(w2i, args)\n",
    "test_sentences = whole_sentences[:3600]\n",
    "train_sentences = whole_sentences[3600:]\n",
    "__train(model, train_sentences)\n",
    "__predict(model, test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [r.prediction_result for r in test_sentences]\n",
    "gold = []\n",
    "for r in test_sentences:\n",
    "    if r.mark:\n",
    "        gold.append(1)\n",
    "    else:\n",
    "        gold.append(0)\n",
    "\n",
    "y_true = np.array(gold)\n",
    "y_scores = np.array(predictions)\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_scores)\n",
    "pr_auc = average_precision_score(y_true, y_scores)  \n",
    "\n",
    "roc_scores.append(roc_auc)\n",
    "pr_scores.append(pr_auc)\n",
    "print(roc_auc, pr_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(10, True, 1)\n",
    "for train, test in kfold.split(whole_sentences):\n",
    "    print('Similarity Experiment')\n",
    "    \n",
    "    model = SimilarityExperiment(w2i, args)\n",
    "    test_sentences = whole_sentences[test]\n",
    "    train_sentences = whole_sentences[train]\n",
    "    \n",
    "    __train(model, train_sentences)\n",
    "    __predict(model, test_sentences)\n",
    "\n",
    "    predictions = [r.prediction_result for r in test_sentences]\n",
    "    gold = []\n",
    "    for r in test_sentences:\n",
    "        if r.mark:\n",
    "            gold.append(1)\n",
    "        else:\n",
    "            gold.append(0)\n",
    "\n",
    "    y_true = np.array(gold)\n",
    "    y_scores = np.array(predictions)\n",
    "\n",
    "    roc_auc = roc_auc_score(y_true, y_scores)\n",
    "    pr_auc = average_precision_score(y_true, y_scores)  \n",
    "    \n",
    "    roc_scores.append(roc_auc)\n",
    "    pr_scores.append(pr_auc)\n",
    "    print(roc_auc, pr_auc)\n",
    "\n",
    "roc_pr_out_dir = os.path.join(model.options.outdir, \"roc_auc.txt\")\n",
    "with open(roc_pr_out_dir, \"w\") as target:\n",
    "    target.write(\"ROC-AUC {}\\n\".format(sum(roc_scores)/len(roc_scores)))\n",
    "    target.write(\"PR-AUC {}\\n\".format(sum(pr_scores)/len(pr_scores)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
